{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "import nfl_data_py as nfl\n",
    "\n",
    "# data loading and plotting\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# models\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor, plot_importance\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# interpretation\n",
    "import shap\n",
    "from interpret import show\n",
    "\n",
    "# pipeline\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.feature_selection import RFECV, RFE\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder, OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, root_mean_squared_error, r2_score, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, nan_euclidean_distances\n",
    "\n",
    "pd.set_option('display.max_columns', None)  # None zeigt alle Spalten\n",
    "pd.set_option('display.max_rows', None)  # Alle Zeilen anzeigen, vorsichtig bei großen DataFrames\n",
    "pd.set_option('display.width', 1000)  # Breite anpassen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positionsspezifische Stats zusammengefasst (passing, rushing und receiving zu total_features zusammengefasst)\n",
    "# Alle Metriken (ewm, mean, median, min, max, std)\n",
    "# --> performed soweit am besten\n",
    "\n",
    "def load_and_merge_data():\n",
    "    df_ids = nfl.import_ids()\n",
    "    df_weekly = nfl.import_weekly_data(list(range(2018, 2025)))\n",
    "    df_seasonal = nfl.import_seasonal_data(list(range(2017,2024)))\n",
    "    df_schedule = nfl.import_schedules(list(range(2018, 2025)))\n",
    "    df_pass_pfr = nfl.import_weekly_pfr('pass', list(range(2018, 2025)))\n",
    "    df_rush_pfr = nfl.import_weekly_pfr('rush', list(range(2018, 2025)))\n",
    "    df_rec_pfr = nfl.import_weekly_pfr('rec', list(range(2018, 2025)))\n",
    "    df_pass_ngs = nfl.import_ngs_data('passing',list(range(2018, 2025)))\n",
    "    df_rush_ngs = nfl.import_ngs_data('rushing',list(range(2018, 2025)))\n",
    "    df_snap_counts = nfl.import_snap_counts(list(range(2018, 2025)))\n",
    "\n",
    "    df_weekly = df_weekly[(df_weekly['season_type'] == 'REG') & (df_weekly['position'].isin(['QB', 'WR', 'RB', 'TE']))].reset_index()\n",
    "\n",
    "    df_seasonal['season'] = df_seasonal['season'] + 1\n",
    "\n",
    "    df_schedule = df_schedule[['game_id', 'home_team', 'home_score', 'away_score']].drop_duplicates()\n",
    "    df_schedule['game_id'] = df_schedule['game_id'].str.replace('OAK', 'LV', regex=False) # Umzug der Oakland Raiders nach Las Vegas in der Saison 2020\n",
    "    df_schedule['home_team'] = df_schedule['home_team'].str.replace('OAK', 'LV', regex=False) # Umzug der Oakland Raiders nach Las Vegas in der Saison 2020\n",
    "\n",
    "    df_weekly['game_id_home_away'] = df_weekly['season'].astype(str) + '_' + df_weekly['week'].apply(lambda x: f\"{x:02d}\")+'_'+df_weekly['recent_team']+'_'+df_weekly['opponent_team']\n",
    "    df_weekly['game_id_away_home'] = df_weekly['season'].astype(str) + '_' + df_weekly['week'].apply(lambda x: f\"{x:02d}\")+'_'+df_weekly['opponent_team']+'_'+df_weekly['recent_team']\n",
    "\n",
    "    df_merged = pd.melt(\n",
    "        df_weekly,\n",
    "        id_vars=['player_id', 'position', 'season', 'week', 'recent_team', 'opponent_team', 'completions', 'attempts', 'passing_yards', 'passing_tds', 'passing_2pt_conversions', 'interceptions', 'sack_fumbles_lost', 'sacks', 'sack_yards', 'passing_air_yards', 'passing_epa', 'pacr', 'carries', 'rushing_yards', 'rushing_tds', 'rushing_2pt_conversions', 'rushing_fumbles_lost', 'rushing_epa', 'receptions', 'targets', 'receiving_yards', 'receiving_tds', 'receiving_2pt_conversions', 'receiving_fumbles_lost', 'racr', 'wopr', 'receiving_epa', 'fantasy_points'],\n",
    "        value_vars=['game_id_home_away', 'game_id_away_home'],\n",
    "        var_name='game_id_type',\n",
    "        value_name='game_id'\n",
    "    )\n",
    "\n",
    "    df_ids = df_ids.rename(columns={'gsis_id': 'player_id', 'pfr_id': 'pfr_player_id'})\n",
    "    df_pass_ngs = df_pass_ngs.rename(columns={'player_gsis_id': 'player_id'})\n",
    "    df_rush_ngs = df_rush_ngs.rename(columns={'player_gsis_id': 'player_id'})\n",
    "\n",
    "    df_merged = pd.merge(df_merged, df_schedule, on='game_id', how='inner') # Bei ein paar Spielen: recent_team = opponent_team\n",
    "    df_merged = pd.merge(df_merged, df_ids[['player_id', 'pfr_player_id', 'draft_pick', 'draft_year']], on = 'player_id', how = 'inner') # Ein paar Spieler ohne draft_year\n",
    "    df_merged = pd.merge(df_merged, df_seasonal[['player_id', 'season', 'dom']], on = ['player_id', 'season'], how = 'left')\n",
    "    df_merged = pd.merge(df_merged, df_pass_pfr[['pfr_player_id', 'season', 'week', 'passing_bad_throws', 'times_pressured']], on = ['pfr_player_id', 'season', 'week'], how = 'left')\n",
    "    df_merged = pd.merge(df_merged, df_rec_pfr[['pfr_player_id', 'season', 'week', 'receiving_rat']], on = ['pfr_player_id', 'season', 'week'], how = 'left')\n",
    "    df_merged = pd.merge(df_merged, df_rush_pfr[['pfr_player_id', 'season', 'week', 'rushing_broken_tackles']], on = ['pfr_player_id', 'season', 'week'], how = 'left')\n",
    "    df_merged = pd.merge(df_merged, df_pass_ngs[['player_id', 'season', 'week', 'passer_rating', 'aggressiveness']], on = ['player_id', 'season', 'week'], how = 'left')\n",
    "    df_merged = pd.merge(df_merged, df_rush_ngs[['player_id', 'season', 'week', 'efficiency']], on = ['player_id', 'season', 'week'], how = 'left')\n",
    "    df_merged = pd.merge(df_merged, df_snap_counts[['pfr_player_id', 'season', 'week', 'offense_snaps']], on = ['pfr_player_id', 'season', 'week'], how = 'left')\n",
    "\n",
    "    df_merged = df_merged.drop(columns=['game_id_type', 'pfr_player_id'])\n",
    "    return df_merged\n",
    "\n",
    "\n",
    "\n",
    "def edit_data(df_merged):\n",
    "    df_merged['draft_pick'] = df_merged['draft_pick'].fillna(260)\n",
    "    df_merged = df_merged.fillna(0)\n",
    "\n",
    "    df_merged['rookie_flag'] = (df_merged['season'] == df_merged['draft_year']).astype(int)\n",
    "    df_merged['last_season_data_flag'] = (df_merged['week'] < 6).astype(int)\n",
    "    df_merged['home'] = (df_merged['home_team'] == df_merged['recent_team']).astype(int)\n",
    "    df_merged['player_id'] = df_merged['player_id'].str.replace('00-00', '').astype(int)\n",
    "\n",
    "    # interceptions und fumbles als eigene features statt als turnover aggregiert\n",
    "    df_merged['turnover'] = (\n",
    "        df_merged['interceptions'] +\n",
    "        df_merged['sack_fumbles_lost'] +\n",
    "        df_merged['rushing_fumbles_lost'] +\n",
    "        df_merged['receiving_fumbles_lost']\n",
    "    )\n",
    "\n",
    "    # total epa aggregiert statt passing, rushing und receiving einzeln\n",
    "    df_merged['epa_total'] = (\n",
    "        df_merged['passing_epa'] + \n",
    "        df_merged['rushing_epa'] + \n",
    "        df_merged['receiving_epa']\n",
    "    )\n",
    "\n",
    "    # total points aggregiert statt passing, rushing und receiving tds und 2pt conversions einzeln\n",
    "    df_merged['points_total'] = (\n",
    "        (df_merged['rushing_tds'] * 6) + \n",
    "        (df_merged['rushing_2pt_conversions'] * 2) + \n",
    "        (df_merged['receiving_tds'] * 6) + \n",
    "        (df_merged['receiving_2pt_conversions'] * 2) + \n",
    "        (df_merged['passing_tds'] * 6) + \n",
    "        (df_merged['passing_2pt_conversions'] * 2)\n",
    "    )\n",
    "\n",
    "    # total yards aggregiert statt passing, rushing und receiving einzeln\n",
    "    df_merged['yards_total'] = (\n",
    "        df_merged['passing_yards'] +\n",
    "        df_merged['rushing_yards'] +\n",
    "        df_merged['receiving_yards']\n",
    "    )\n",
    "\n",
    "    # position target-encoded\n",
    "    position_means = df_merged.groupby(['position', 'season', 'week'])['fantasy_points'].mean().reset_index()\n",
    "    position_means.rename(columns={'fantasy_points': 'position_encoded'}, inplace=True)\n",
    "    df_merged = pd.merge(df_merged, position_means, on=['position', 'season', 'week'], how='left')\n",
    "\n",
    "    return df_merged\n",
    "\n",
    "\n",
    "\n",
    "def create_rolling_features(df_merged):  \n",
    "\n",
    "    # points_scored und points_allowed als Maßstab für Stärke eines Teams\n",
    "    df_merged['recent_team_points_scored'] = df_merged.apply(lambda row: row['home_score'] if row['home'] == 1 else row['away_score'], axis=1)\n",
    "    df_merged['opponent_team_points_allowed'] = df_merged['recent_team_points_scored']\n",
    "\n",
    "    df_unique_opponent_team_points_allowed = df_merged.drop_duplicates(subset=['game_id', 'opponent_team', 'opponent_team_points_allowed'])\n",
    "    df_unique_recent_team_points_scored = df_merged.drop_duplicates(subset=['game_id', 'recent_team', 'recent_team_points_scored'])\n",
    "\n",
    "    df_unique_opponent_team_points_allowed = df_unique_opponent_team_points_allowed.sort_values(by=['opponent_team', 'season', 'week']).reset_index(drop=True)\n",
    "    df_unique_recent_team_points_scored = df_unique_recent_team_points_scored.sort_values(by=['recent_team', 'season', 'week']).reset_index(drop=True)\n",
    "\n",
    "    df_unique_opponent_team_points_allowed['ewm_opponent_team_points_allowed_l5w'] = (\n",
    "        df_unique_opponent_team_points_allowed.groupby('opponent_team')['opponent_team_points_allowed']\n",
    "        .apply(lambda x: x.shift(1).ewm(span=5, min_periods=5).mean())\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n",
    "    for metric in ['mean', 'median', 'std']:\n",
    "            df_unique_opponent_team_points_allowed[f\"{metric}_opponent_team_points_allowed_l5w\"] = (\n",
    "                df_unique_opponent_team_points_allowed.groupby('opponent_team')['opponent_team_points_allowed']\n",
    "                .apply(lambda x: x.shift(1).rolling(window=5, min_periods=5).agg(metric))  # shift(1) schließt aktuelle Woche aus\n",
    "                .reset_index(level=0, drop=True)  # Index zurücksetzen\n",
    "        )\n",
    "\n",
    "    for metric in ['min', 'max']:\n",
    "            df_unique_opponent_team_points_allowed[f\"{metric}_opponent_team_points_allowed_l3w\"] = (\n",
    "                df_unique_opponent_team_points_allowed.groupby('opponent_team')['opponent_team_points_allowed']\n",
    "                .apply(lambda x: x.shift(1).rolling(window=3, min_periods=3).agg(metric))  # shift(1) schließt aktuelle Woche aus\n",
    "                .reset_index(level=0, drop=True)  # Index zurücksetzen\n",
    "        )\n",
    "\n",
    "    df_unique_opponent_team_points_allowed = df_unique_opponent_team_points_allowed.drop(columns=['player_id', 'draft_year', 'turnover', 'interceptions', 'sack_fumbles_lost', 'rushing_fumbles_lost', 'receiving_fumbles_lost', 'points_total', 'rushing_tds', 'rushing_2pt_conversions', 'receiving_tds', 'receiving_2pt_conversions', 'passing_tds', 'passing_2pt_conversions', 'epa_total', 'passing_epa', 'rushing_epa', 'receiving_epa', 'position', 'season', 'week', 'recent_team', 'home_team', 'completions', 'attempts', 'passing_yards', 'sacks', 'sack_yards', 'passing_air_yards', 'pacr', 'carries', 'rushing_yards', 'receptions', 'targets', 'yards_total', 'receiving_yards', 'racr', 'wopr', 'fantasy_points', 'home_score', 'away_score', 'draft_pick', 'dom', 'passing_bad_throws', 'times_pressured', 'receiving_rat', 'rushing_broken_tackles', 'passer_rating', 'aggressiveness', 'efficiency', 'offense_snaps', 'rookie_flag', 'last_season_data_flag', 'home', 'position_encoded', 'recent_team_points_scored', 'opponent_team_points_allowed'])\n",
    "    df_merged = pd.merge(df_merged, df_unique_opponent_team_points_allowed, on=['game_id','opponent_team'], how='inner')\n",
    "\n",
    "    df_unique_recent_team_points_scored['ewm_recent_team_points_scored_l5w'] = (\n",
    "        df_unique_recent_team_points_scored.groupby('recent_team')['recent_team_points_scored']\n",
    "        .apply(lambda x: x.shift(1).ewm(span=5, min_periods=5).mean())\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n",
    "    for metric in ['mean', 'median', 'std']:\n",
    "            df_unique_recent_team_points_scored[f\"{metric}_recent_team_points_scored_l5w\"] = (\n",
    "                df_unique_recent_team_points_scored.groupby('recent_team')['recent_team_points_scored']\n",
    "                .apply(lambda x: x.shift(1).rolling(window=5, min_periods=5).agg(metric))  # shift(1) schließt aktuelle Woche aus\n",
    "                .reset_index(level=0, drop=True)  # Index zurücksetzen\n",
    "        )\n",
    "\n",
    "    for metric in ['min', 'max']:\n",
    "            df_unique_recent_team_points_scored[f\"{metric}_recent_team_points_scored_l3w\"] = (\n",
    "                df_unique_recent_team_points_scored.groupby('recent_team')['recent_team_points_scored']\n",
    "                .apply(lambda x: x.shift(1).rolling(window=3, min_periods=3).agg(metric))  # shift(1) schließt aktuelle Woche aus\n",
    "                .reset_index(level=0, drop=True)  # Index zurücksetzen\n",
    "        )\n",
    "\n",
    "    df_unique_recent_team_points_scored = df_unique_recent_team_points_scored.drop(columns=['player_id', 'draft_year', 'turnover', 'interceptions', 'sack_fumbles_lost', 'rushing_fumbles_lost', 'receiving_fumbles_lost', 'points_total', 'rushing_tds', 'rushing_2pt_conversions', 'receiving_tds', 'receiving_2pt_conversions', 'passing_tds', 'passing_2pt_conversions', 'epa_total', 'passing_epa', 'rushing_epa', 'receiving_epa', 'position', 'season', 'week', 'opponent_team', 'home_team', 'completions', 'attempts', 'yards_total', 'passing_yards', 'sacks', 'sack_yards', 'passing_air_yards', 'pacr', 'carries', 'rushing_yards', 'receptions', 'targets', 'receiving_yards', 'racr', 'wopr', 'fantasy_points', 'home_score', 'away_score', 'draft_pick', 'dom', 'passing_bad_throws', 'times_pressured', 'receiving_rat', 'rushing_broken_tackles', 'passer_rating', 'aggressiveness', 'efficiency', 'offense_snaps', 'rookie_flag', 'last_season_data_flag', 'home', 'position_encoded', 'recent_team_points_scored', 'opponent_team_points_allowed'])\n",
    "    df_merged = pd.merge(df_merged, df_unique_recent_team_points_scored, on=['game_id','recent_team'], how='inner')\n",
    "\n",
    "    # Liste der Spalten mit Spielerspezifischen numerischen Daten, für die Rolling-Features erstellt werden sollen\n",
    "    columns_to_roll = ['completions', 'attempts', 'sacks', 'passer_rating', 'aggressiveness', 'efficiency', 'sack_yards', \n",
    "                    'passing_air_yards', 'pacr', 'carries', 'offense_snaps', 'yards_total', 'receptions', 'targets',\n",
    "                    'racr', 'wopr', 'fantasy_points', 'passing_bad_throws', 'times_pressured', 'position_encoded', 'receiving_rat', \n",
    "                    'rushing_broken_tackles', 'turnover', 'points_total', 'epa_total']\n",
    "\n",
    "\n",
    "    # Sortiere nach player_id, season und week\n",
    "    df_merged = df_merged.sort_values(by=['player_id', 'season', 'week']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    df_merged['cnt_games_over_20ffpts_l5w'] = (\n",
    "        df_merged.groupby('player_id')['fantasy_points']\n",
    "        .apply(lambda x: x.shift(1).rolling(window=5, min_periods=5).apply(lambda y: (y > 20).sum()))\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n",
    "    # Rolling-Features erstellen\n",
    "    for col in columns_to_roll:\n",
    "\n",
    "        feature_name_1 = f\"ewm_{col}_l5w\"\n",
    "        df_merged[feature_name_1] = (\n",
    "            df_merged.groupby('player_id')[col]\n",
    "            .apply(lambda x: x.shift(1).ewm(span=5, min_periods=5).mean())\n",
    "            .reset_index(level=0, drop=True)\n",
    "        )\n",
    "\n",
    "        for metric in ['mean', 'median', 'std']:\n",
    "            feature_name_2 = f\"{metric}_{col}_l5w\"\n",
    "            rolling_result_5w = (\n",
    "                df_merged.groupby('player_id')[col]\n",
    "                    .apply(lambda x: x.shift(1).rolling(window=5, min_periods=5).agg(metric))  # shift(1) schließt aktuelle Woche aus\n",
    "                    .reset_index(level=0, drop=True)  # Index zurücksetzen\n",
    "            )\n",
    "            # Einfügen der Rolling-Metrik\n",
    "            df_merged[feature_name_2] = rolling_result_5w\n",
    "\n",
    "        for metric in ['max', 'min']:\n",
    "            feature_name_3 = f\"{metric}_{col}_l3w\"\n",
    "            # Berechnung der Rolling-Metrik (ohne aktuelle Woche)\n",
    "            rolling_result_3w = (\n",
    "                df_merged.groupby('player_id')[col]\n",
    "                    .apply(lambda x: x.shift(1).rolling(window=3, min_periods=3).agg(metric))  # shift(1) schließt aktuelle Woche aus\n",
    "                    .reset_index(level=0, drop=True)  # Index zurücksetzen\n",
    "            )\n",
    "            # Einfügen der Rolling-Metrik\n",
    "            df_merged[feature_name_3] = rolling_result_3w\n",
    "\n",
    "    df_merged = df_merged.drop(columns=['completions', 'attempts', 'passing_yards', 'sacks', 'sack_yards', 'passing_air_yards', 'pacr', 'carries', \n",
    "                                        'rushing_yards', 'receptions', 'targets', 'receiving_yards', 'racr', 'wopr', 'passing_bad_throws', \n",
    "                                        'times_pressured', 'receiving_rat', 'rushing_broken_tackles', 'draft_year', 'home_team', \n",
    "                                        'passer_rating', 'aggressiveness', 'efficiency', 'offense_snaps', 'game_id',  'interceptions', 'sack_fumbles_lost', \n",
    "                                        'rushing_fumbles_lost', 'receiving_fumbles_lost', 'rushing_tds', 'rushing_2pt_conversions', 'receiving_tds', \n",
    "                                        'receiving_2pt_conversions', 'passing_tds', 'passing_2pt_conversions', 'passing_epa', 'rushing_epa', 'receiving_epa',\n",
    "                                        'position_encoded', 'recent_team', 'opponent_team', 'position', 'home_score', 'away_score',\n",
    "                                        'recent_team_points_scored', 'opponent_team_points_allowed', 'turnover', 'points_total', 'yards_total',\n",
    "                                        'epa_total'])\n",
    "\n",
    "    df_merged = df_merged.dropna().reset_index(level=0, drop=True)\n",
    "\n",
    "    return df_merged\n",
    "\n",
    "\n",
    "\n",
    "def prepare_features():  \n",
    "\n",
    "    df_merged = load_and_merge_data()\n",
    "    df_merged = edit_data(df_merged)\n",
    "    df_merged = create_rolling_features(df_merged)\n",
    "\n",
    "    return df_merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df_merged):\n",
    "\n",
    "    X_train = df_merged[df_merged['season'].isin(list(range(2018, 2024)))].drop(columns=['fantasy_points'])\n",
    "    y_train = df_merged[df_merged['season'].isin(list(range(2018, 2024)))]['fantasy_points']\n",
    "\n",
    "    X_val = df_merged[df_merged['season']==2023].drop(columns=['fantasy_points'])\n",
    "    y_val = df_merged[df_merged['season']==2023]['fantasy_points']\n",
    "\n",
    "    X_test = df_merged[df_merged['season']==2024].drop(columns=['fantasy_points'])\n",
    "    y_test = df_merged[df_merged['season']==2024]['fantasy_points']\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_preprocessor(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    numeric_features = X_train.select_dtypes(include=['int32', 'int64', 'float32', 'float64']).columns\n",
    "    categorical_features = X_train.select_dtypes(include=['object']).columns\n",
    "\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return preprocessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_pipeline(X_train, X_test, y_train, y_test, preprocessor):\n",
    "    # Modelle definieren\n",
    "    models = {\n",
    "        \"LinearRegression\": LinearRegression(),\n",
    "        \"XGBoost\": XGBRegressor()\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        # Pipeline erstellen\n",
    "        pipeline = Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('model', model)\n",
    "        ])\n",
    "        \n",
    "        # Training\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        # Vorhersagen\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "        \n",
    "        # Evaluation\n",
    "        results[model_name] = {\n",
    "            \"R2\": r2_score(y_test, y_pred),\n",
    "            \"MAE\": mean_absolute_error(y_test, y_pred),\n",
    "            \"RMSE\": np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_grid_search(X_train, y_train, preprocessor, model_name=\"XGBoost\"):\n",
    "    \n",
    "    # Hyperparameter für XGBoost\n",
    "    xgb_grid = {\n",
    "        \"model__max_depth\": [3, 6, 7, None],\n",
    "        \"model__learning_rate\": [0.01, 0.05, 0.1, 0.3],\n",
    "        \"model__n_estimators\": [50, 100, 200, 500, 1000]\n",
    "    }\n",
    "    \n",
    "    # Pipeline erstellen\n",
    "    model = XGBRegressor()\n",
    "    \n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    # GridSearchCV\n",
    "    grid_search = GridSearchCV(pipeline, param_grid=xgb_grid, verbose=2, cv=TimeSeriesSplit(n_splits=3))\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    bestModel = grid_search.best_estimator_.fit(X_train, y_train)\n",
    "    \n",
    "    return bestModel.best_params_, bestModel.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_models(X_test, y_test):\n",
    "\n",
    "    # Modelle und ihre Namen\n",
    "    models = {\n",
    "        'Linear Regression': lr,\n",
    "        'Explainable Boosting Machine': ebm,\n",
    "        'Multi Layer Perceptron': mlp,\n",
    "        'XGBoost': xgb\n",
    "    }\n",
    "    \n",
    "    # Ausgabe der Metriken\n",
    "    print(\"\\nModel Evaluation Results:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f'\\n{name}')\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Berechnung der Metriken\n",
    "        metrics = {\n",
    "            'Mean Absolute Error': mean_absolute_error(y_test, y_pred),\n",
    "            'Mean Squared Error': mean_squared_error(y_test, y_pred),\n",
    "            'Root Mean Squared Error': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "            'R² Score': r2_score(y_test, y_pred)\n",
    "        }\n",
    "        \n",
    "        # Ausgabe der Metriken\n",
    "        for metric_name, value in metrics.items():\n",
    "            print(f'    {metric_name:25}: {value:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downcasting floats.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sn/54gnf37x14l2fsfqr_hj0dnc0000gn/T/ipykernel_12085/1399416720.py:209: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_merged[feature_name_3] = rolling_result_3w\n",
      "/var/folders/sn/54gnf37x14l2fsfqr_hj0dnc0000gn/T/ipykernel_12085/1399416720.py:209: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_merged[feature_name_3] = rolling_result_3w\n",
      "/var/folders/sn/54gnf37x14l2fsfqr_hj0dnc0000gn/T/ipykernel_12085/1399416720.py:184: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_merged[feature_name_1] = (\n",
      "/var/folders/sn/54gnf37x14l2fsfqr_hj0dnc0000gn/T/ipykernel_12085/1399416720.py:198: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_merged[feature_name_2] = rolling_result_5w\n",
      "/var/folders/sn/54gnf37x14l2fsfqr_hj0dnc0000gn/T/ipykernel_12085/1399416720.py:198: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_merged[feature_name_2] = rolling_result_5w\n",
      "/var/folders/sn/54gnf37x14l2fsfqr_hj0dnc0000gn/T/ipykernel_12085/1399416720.py:198: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_merged[feature_name_2] = rolling_result_5w\n",
      "/var/folders/sn/54gnf37x14l2fsfqr_hj0dnc0000gn/T/ipykernel_12085/1399416720.py:209: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_merged[feature_name_3] = rolling_result_3w\n",
      "/var/folders/sn/54gnf37x14l2fsfqr_hj0dnc0000gn/T/ipykernel_12085/1399416720.py:209: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_merged[feature_name_3] = rolling_result_3w\n",
      "/var/folders/sn/54gnf37x14l2fsfqr_hj0dnc0000gn/T/ipykernel_12085/1399416720.py:184: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_merged[feature_name_1] = (\n",
      "/var/folders/sn/54gnf37x14l2fsfqr_hj0dnc0000gn/T/ipykernel_12085/1399416720.py:198: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_merged[feature_name_2] = rolling_result_5w\n",
      "/var/folders/sn/54gnf37x14l2fsfqr_hj0dnc0000gn/T/ipykernel_12085/1399416720.py:198: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_merged[feature_name_2] = rolling_result_5w\n",
      "/var/folders/sn/54gnf37x14l2fsfqr_hj0dnc0000gn/T/ipykernel_12085/1399416720.py:198: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_merged[feature_name_2] = rolling_result_5w\n",
      "/var/folders/sn/54gnf37x14l2fsfqr_hj0dnc0000gn/T/ipykernel_12085/1399416720.py:209: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_merged[feature_name_3] = rolling_result_3w\n",
      "/var/folders/sn/54gnf37x14l2fsfqr_hj0dnc0000gn/T/ipykernel_12085/1399416720.py:209: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_merged[feature_name_3] = rolling_result_3w\n",
      "/var/folders/sn/54gnf37x14l2fsfqr_hj0dnc0000gn/T/ipykernel_12085/1399416720.py:184: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_merged[feature_name_1] = (\n",
      "/var/folders/sn/54gnf37x14l2fsfqr_hj0dnc0000gn/T/ipykernel_12085/1399416720.py:198: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_merged[feature_name_2] = rolling_result_5w\n",
      "/var/folders/sn/54gnf37x14l2fsfqr_hj0dnc0000gn/T/ipykernel_12085/1399416720.py:198: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_merged[feature_name_2] = rolling_result_5w\n",
      "/var/folders/sn/54gnf37x14l2fsfqr_hj0dnc0000gn/T/ipykernel_12085/1399416720.py:198: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_merged[feature_name_2] = rolling_result_5w\n",
      "/var/folders/sn/54gnf37x14l2fsfqr_hj0dnc0000gn/T/ipykernel_12085/1399416720.py:209: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_merged[feature_name_3] = rolling_result_3w\n",
      "/var/folders/sn/54gnf37x14l2fsfqr_hj0dnc0000gn/T/ipykernel_12085/1399416720.py:209: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_merged[feature_name_3] = rolling_result_3w\n",
      "/var/folders/sn/54gnf37x14l2fsfqr_hj0dnc0000gn/T/ipykernel_12085/1399416720.py:184: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_merged[feature_name_1] = (\n",
      "/var/folders/sn/54gnf37x14l2fsfqr_hj0dnc0000gn/T/ipykernel_12085/1399416720.py:198: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_merged[feature_name_2] = rolling_result_5w\n",
      "/var/folders/sn/54gnf37x14l2fsfqr_hj0dnc0000gn/T/ipykernel_12085/1399416720.py:198: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_merged[feature_name_2] = rolling_result_5w\n",
      "/var/folders/sn/54gnf37x14l2fsfqr_hj0dnc0000gn/T/ipykernel_12085/1399416720.py:198: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_merged[feature_name_2] = rolling_result_5w\n",
      "/var/folders/sn/54gnf37x14l2fsfqr_hj0dnc0000gn/T/ipykernel_12085/1399416720.py:209: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_merged[feature_name_3] = rolling_result_3w\n",
      "/var/folders/sn/54gnf37x14l2fsfqr_hj0dnc0000gn/T/ipykernel_12085/1399416720.py:209: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_merged[feature_name_3] = rolling_result_3w\n",
      "/var/folders/sn/54gnf37x14l2fsfqr_hj0dnc0000gn/T/ipykernel_12085/1399416720.py:184: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_merged[feature_name_1] = (\n",
      "/var/folders/sn/54gnf37x14l2fsfqr_hj0dnc0000gn/T/ipykernel_12085/1399416720.py:198: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_merged[feature_name_2] = rolling_result_5w\n",
      "/var/folders/sn/54gnf37x14l2fsfqr_hj0dnc0000gn/T/ipykernel_12085/1399416720.py:198: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_merged[feature_name_2] = rolling_result_5w\n",
      "/var/folders/sn/54gnf37x14l2fsfqr_hj0dnc0000gn/T/ipykernel_12085/1399416720.py:198: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_merged[feature_name_2] = rolling_result_5w\n",
      "/var/folders/sn/54gnf37x14l2fsfqr_hj0dnc0000gn/T/ipykernel_12085/1399416720.py:209: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_merged[feature_name_3] = rolling_result_3w\n",
      "/var/folders/sn/54gnf37x14l2fsfqr_hj0dnc0000gn/T/ipykernel_12085/1399416720.py:209: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_merged[feature_name_3] = rolling_result_3w\n",
      "/var/folders/sn/54gnf37x14l2fsfqr_hj0dnc0000gn/T/ipykernel_12085/1399416720.py:184: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_merged[feature_name_1] = (\n",
      "/var/folders/sn/54gnf37x14l2fsfqr_hj0dnc0000gn/T/ipykernel_12085/1399416720.py:198: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_merged[feature_name_2] = rolling_result_5w\n",
      "/var/folders/sn/54gnf37x14l2fsfqr_hj0dnc0000gn/T/ipykernel_12085/1399416720.py:198: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_merged[feature_name_2] = rolling_result_5w\n",
      "/var/folders/sn/54gnf37x14l2fsfqr_hj0dnc0000gn/T/ipykernel_12085/1399416720.py:198: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_merged[feature_name_2] = rolling_result_5w\n",
      "/var/folders/sn/54gnf37x14l2fsfqr_hj0dnc0000gn/T/ipykernel_12085/1399416720.py:209: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_merged[feature_name_3] = rolling_result_3w\n",
      "/var/folders/sn/54gnf37x14l2fsfqr_hj0dnc0000gn/T/ipykernel_12085/1399416720.py:209: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_merged[feature_name_3] = rolling_result_3w\n",
      "/var/folders/sn/54gnf37x14l2fsfqr_hj0dnc0000gn/T/ipykernel_12085/1399416720.py:184: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_merged[feature_name_1] = (\n",
      "/var/folders/sn/54gnf37x14l2fsfqr_hj0dnc0000gn/T/ipykernel_12085/1399416720.py:198: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_merged[feature_name_2] = rolling_result_5w\n",
      "/var/folders/sn/54gnf37x14l2fsfqr_hj0dnc0000gn/T/ipykernel_12085/1399416720.py:198: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_merged[feature_name_2] = rolling_result_5w\n",
      "/var/folders/sn/54gnf37x14l2fsfqr_hj0dnc0000gn/T/ipykernel_12085/1399416720.py:198: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_merged[feature_name_2] = rolling_result_5w\n",
      "/var/folders/sn/54gnf37x14l2fsfqr_hj0dnc0000gn/T/ipykernel_12085/1399416720.py:209: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_merged[feature_name_3] = rolling_result_3w\n",
      "/var/folders/sn/54gnf37x14l2fsfqr_hj0dnc0000gn/T/ipykernel_12085/1399416720.py:209: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_merged[feature_name_3] = rolling_result_3w\n",
      "/var/folders/sn/54gnf37x14l2fsfqr_hj0dnc0000gn/T/ipykernel_12085/1399416720.py:184: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_merged[feature_name_1] = (\n",
      "/var/folders/sn/54gnf37x14l2fsfqr_hj0dnc0000gn/T/ipykernel_12085/1399416720.py:198: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_merged[feature_name_2] = rolling_result_5w\n",
      "/var/folders/sn/54gnf37x14l2fsfqr_hj0dnc0000gn/T/ipykernel_12085/1399416720.py:198: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_merged[feature_name_2] = rolling_result_5w\n",
      "/var/folders/sn/54gnf37x14l2fsfqr_hj0dnc0000gn/T/ipykernel_12085/1399416720.py:198: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_merged[feature_name_2] = rolling_result_5w\n",
      "/var/folders/sn/54gnf37x14l2fsfqr_hj0dnc0000gn/T/ipykernel_12085/1399416720.py:209: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_merged[feature_name_3] = rolling_result_3w\n",
      "/var/folders/sn/54gnf37x14l2fsfqr_hj0dnc0000gn/T/ipykernel_12085/1399416720.py:209: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_merged[feature_name_3] = rolling_result_3w\n",
      "/var/folders/sn/54gnf37x14l2fsfqr_hj0dnc0000gn/T/ipykernel_12085/1399416720.py:184: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_merged[feature_name_1] = (\n",
      "/var/folders/sn/54gnf37x14l2fsfqr_hj0dnc0000gn/T/ipykernel_12085/1399416720.py:198: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_merged[feature_name_2] = rolling_result_5w\n",
      "/var/folders/sn/54gnf37x14l2fsfqr_hj0dnc0000gn/T/ipykernel_12085/1399416720.py:198: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_merged[feature_name_2] = rolling_result_5w\n",
      "/var/folders/sn/54gnf37x14l2fsfqr_hj0dnc0000gn/T/ipykernel_12085/1399416720.py:198: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_merged[feature_name_2] = rolling_result_5w\n",
      "/var/folders/sn/54gnf37x14l2fsfqr_hj0dnc0000gn/T/ipykernel_12085/1399416720.py:209: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_merged[feature_name_3] = rolling_result_3w\n",
      "/var/folders/sn/54gnf37x14l2fsfqr_hj0dnc0000gn/T/ipykernel_12085/1399416720.py:209: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_merged[feature_name_3] = rolling_result_3w\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluationsergebnisse: {'LinearRegression': {'R2': 0.39032808540322195, 'MAE': np.float64(4.066928707586389), 'RMSE': np.float64(5.606913251675447)}, 'XGBoost': {'R2': 0.32895082235336304, 'MAE': np.float32(4.2860394), 'RMSE': np.float32(5.882378)}}\n",
      "Fitting 3 folds for each of 80 candidates, totalling 240 fits\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=50; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=50; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=50; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100; total time=   0.3s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=100; total time=   0.4s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200; total time=   0.4s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200; total time=   0.5s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=200; total time=   0.6s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=500; total time=   0.8s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=500; total time=   1.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=500; total time=   1.3s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=1000; total time=   1.5s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=1000; total time=   1.9s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=3, model__n_estimators=1000; total time=   2.3s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=6, model__n_estimators=50; total time=   0.4s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=6, model__n_estimators=50; total time=   0.4s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=6, model__n_estimators=50; total time=   0.5s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=6, model__n_estimators=100; total time=   0.6s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=6, model__n_estimators=100; total time=   0.7s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=6, model__n_estimators=100; total time=   0.8s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=6, model__n_estimators=200; total time=   1.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=6, model__n_estimators=200; total time=   1.4s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=6, model__n_estimators=200; total time=   1.7s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=6, model__n_estimators=500; total time=   2.5s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=6, model__n_estimators=500; total time=   3.0s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=6, model__n_estimators=500; total time=   3.4s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=6, model__n_estimators=1000; total time=   4.8s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=6, model__n_estimators=1000; total time=   5.4s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=6, model__n_estimators=1000; total time=   5.8s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=50; total time=   0.5s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=50; total time=   0.6s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=50; total time=   0.7s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=100; total time=   0.9s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=100; total time=   1.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=100; total time=   1.2s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=200; total time=   1.7s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=200; total time=   2.2s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=200; total time=   2.3s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=500; total time=   3.8s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=500; total time=   4.7s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=500; total time=   5.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=1000; total time=   7.4s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=1000; total time=   8.8s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=7, model__n_estimators=1000; total time=   9.1s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=None, model__n_estimators=50; total time=   0.3s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=None, model__n_estimators=50; total time=   0.4s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=None, model__n_estimators=50; total time=   0.4s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=None, model__n_estimators=100; total time=   0.6s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=None, model__n_estimators=100; total time=   0.7s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=None, model__n_estimators=100; total time=   0.9s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=None, model__n_estimators=200; total time=   1.2s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=None, model__n_estimators=200; total time=   1.4s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=None, model__n_estimators=200; total time=   1.5s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=None, model__n_estimators=500; total time=   2.6s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=None, model__n_estimators=500; total time=   3.0s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=None, model__n_estimators=500; total time=   3.3s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=None, model__n_estimators=1000; total time=   4.9s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=None, model__n_estimators=1000; total time=   5.9s\n",
      "[CV] END model__learning_rate=0.01, model__max_depth=None, model__n_estimators=1000; total time=   6.6s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=3, model__n_estimators=50; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=3, model__n_estimators=50; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=3, model__n_estimators=50; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=3, model__n_estimators=100; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=3, model__n_estimators=100; total time=   0.3s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=3, model__n_estimators=100; total time=   0.4s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=3, model__n_estimators=200; total time=   0.4s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=3, model__n_estimators=200; total time=   0.5s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=3, model__n_estimators=200; total time=   0.6s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=3, model__n_estimators=500; total time=   0.9s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=3, model__n_estimators=500; total time=   1.2s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=3, model__n_estimators=500; total time=   1.3s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=3, model__n_estimators=1000; total time=   1.7s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=3, model__n_estimators=1000; total time=   2.1s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=3, model__n_estimators=1000; total time=   2.6s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=6, model__n_estimators=50; total time=   0.4s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=6, model__n_estimators=50; total time=   0.5s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=6, model__n_estimators=50; total time=   0.5s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=6, model__n_estimators=100; total time=   0.6s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=6, model__n_estimators=100; total time=   0.8s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=6, model__n_estimators=100; total time=   0.8s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=6, model__n_estimators=200; total time=   1.0s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=6, model__n_estimators=200; total time=   1.1s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=6, model__n_estimators=200; total time=   1.3s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=6, model__n_estimators=500; total time=   2.9s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=6, model__n_estimators=500; total time=   2.7s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=6, model__n_estimators=500; total time=   3.0s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=6, model__n_estimators=1000; total time=   4.8s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=6, model__n_estimators=1000; total time=   5.8s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=6, model__n_estimators=1000; total time=   6.0s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=7, model__n_estimators=50; total time=   0.5s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=7, model__n_estimators=50; total time=   0.6s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=7, model__n_estimators=50; total time=   0.6s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=7, model__n_estimators=100; total time=   0.8s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=7, model__n_estimators=100; total time=   1.0s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=7, model__n_estimators=100; total time=   1.1s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=7, model__n_estimators=200; total time=   1.6s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=7, model__n_estimators=200; total time=   1.6s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=7, model__n_estimators=200; total time=   1.8s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=7, model__n_estimators=500; total time=   3.5s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=7, model__n_estimators=500; total time=   3.9s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=7, model__n_estimators=500; total time=   4.4s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=7, model__n_estimators=1000; total time=   7.1s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=7, model__n_estimators=1000; total time=   8.4s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=7, model__n_estimators=1000; total time=   8.8s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=None, model__n_estimators=50; total time=   0.3s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=None, model__n_estimators=50; total time=   0.4s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=None, model__n_estimators=50; total time=   0.4s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=None, model__n_estimators=100; total time=   0.5s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=None, model__n_estimators=100; total time=   0.7s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=None, model__n_estimators=100; total time=   0.8s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=None, model__n_estimators=200; total time=   1.1s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=None, model__n_estimators=200; total time=   1.2s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=None, model__n_estimators=200; total time=   1.3s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=None, model__n_estimators=500; total time=   2.4s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=None, model__n_estimators=500; total time=   2.7s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=None, model__n_estimators=500; total time=   3.0s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=None, model__n_estimators=1000; total time=   4.8s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=None, model__n_estimators=1000; total time=   5.5s\n",
      "[CV] END model__learning_rate=0.05, model__max_depth=None, model__n_estimators=1000; total time=   5.9s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=50; total time=   0.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=50; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=50; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100; total time=   0.2s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=100; total time=   0.3s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200; total time=   0.4s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200; total time=   0.4s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=200; total time=   0.5s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=500; total time=   0.8s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=500; total time=   0.9s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=500; total time=   1.2s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=1000; total time=   1.7s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=1000; total time=   2.1s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=3, model__n_estimators=1000; total time=   2.4s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=6, model__n_estimators=50; total time=   0.4s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=6, model__n_estimators=50; total time=   0.4s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=6, model__n_estimators=50; total time=   0.5s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=6, model__n_estimators=100; total time=   0.6s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=6, model__n_estimators=100; total time=   0.7s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=6, model__n_estimators=100; total time=   0.7s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=6, model__n_estimators=200; total time=   1.4s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=6, model__n_estimators=200; total time=   1.6s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=6, model__n_estimators=200; total time=   1.7s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=6, model__n_estimators=500; total time=   2.6s\n",
      "[CV] END model__learning_rate=0.1, model__max_depth=6, model__n_estimators=500; total time=   3.0s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Pipeline starten\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 21\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[35], line 15\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluationsergebnisse:\u001b[39m\u001b[38;5;124m\"\u001b[39m, results)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Hyperparameter-Tuning\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m best_params, best_score \u001b[38;5;241m=\u001b[39m \u001b[43mperform_grid_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocessor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBeste Hyperparameter:\u001b[39m\u001b[38;5;124m\"\u001b[39m, best_params)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBestes Ergebnis (neg. MSE):\u001b[39m\u001b[38;5;124m\"\u001b[39m, best_score)\n",
      "Cell \u001b[0;32mIn[34], line 20\u001b[0m, in \u001b[0;36mperform_grid_search\u001b[0;34m(X_train, y_train, preprocessor, model_name)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# GridSearchCV\u001b[39;00m\n\u001b[1;32m     19\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(pipeline, param_grid\u001b[38;5;241m=\u001b[39mxgb_grid, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, cv\u001b[38;5;241m=\u001b[39mTimeSeriesSplit(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m---> 20\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m bestModel \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m bestModel\u001b[38;5;241m.\u001b[39mbest_params_, bestModel\u001b[38;5;241m.\u001b[39mbest_score_\n",
      "File \u001b[0;32m~/Desktop/Uni/Master/WiSe_24-25/DSAI/Projekt/venv/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Uni/Master/WiSe_24-25/DSAI/Projekt/venv/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1019\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m   1014\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m   1015\u001b[0m     )\n\u001b[1;32m   1017\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m-> 1019\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m   1023\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/Uni/Master/WiSe_24-25/DSAI/Projekt/venv/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1573\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1571\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1572\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1573\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Uni/Master/WiSe_24-25/DSAI/Projekt/venv/lib/python3.12/site-packages/sklearn/model_selection/_search.py:965\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    959\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    960\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    961\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    962\u001b[0m         )\n\u001b[1;32m    963\u001b[0m     )\n\u001b[0;32m--> 965\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    984\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    985\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    987\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    988\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/Uni/Master/WiSe_24-25/DSAI/Projekt/venv/lib/python3.12/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Uni/Master/WiSe_24-25/DSAI/Projekt/venv/lib/python3.12/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/Desktop/Uni/Master/WiSe_24-25/DSAI/Projekt/venv/lib/python3.12/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/Desktop/Uni/Master/WiSe_24-25/DSAI/Projekt/venv/lib/python3.12/site-packages/sklearn/utils/parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Uni/Master/WiSe_24-25/DSAI/Projekt/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:888\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    886\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    887\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 888\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    891\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    892\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/Desktop/Uni/Master/WiSe_24-25/DSAI/Projekt/venv/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Uni/Master/WiSe_24-25/DSAI/Projekt/venv/lib/python3.12/site-packages/sklearn/pipeline.py:473\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    472\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m routed_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m--> 473\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/Uni/Master/WiSe_24-25/DSAI/Projekt/venv/lib/python3.12/site-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Uni/Master/WiSe_24-25/DSAI/Projekt/venv/lib/python3.12/site-packages/xgboost/sklearn.py:1108\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1105\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m model, metric, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(xgb_model, params)\n\u001b[0;32m-> 1108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_evaluation_result(evals_result)\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/Uni/Master/WiSe_24-25/DSAI/Projekt/venv/lib/python3.12/site-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Uni/Master/WiSe_24-25/DSAI/Projekt/venv/lib/python3.12/site-packages/xgboost/training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Uni/Master/WiSe_24-25/DSAI/Projekt/venv/lib/python3.12/site-packages/xgboost/core.py:2101\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[1;32m   2099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2100\u001b[0m     _check_call(\n\u001b[0;32m-> 2101\u001b[0m         \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2102\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\n\u001b[1;32m   2103\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2104\u001b[0m     )\n\u001b[1;32m   2105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2106\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    # Daten laden und aufteilen\n",
    "    df = prepare_features()\n",
    "    X_train, X_test, y_train, y_test = split_data(df)\n",
    "    \n",
    "    # Preprocessor erstellen\n",
    "    preprocessor = create_preprocessor(X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    # Modelle trainieren und evaluieren\n",
    "    results = train_and_evaluate_pipeline(X_train, X_test, y_train, y_test, preprocessor)\n",
    "    print(\"Evaluationsergebnisse:\", results)\n",
    "    \n",
    "    # Hyperparameter-Tuning\n",
    "    best_params, best_score = perform_grid_search(X_train, y_train, preprocessor)\n",
    "    print(\"Beste Hyperparameter:\", best_params)\n",
    "    print(\"Bestes Ergebnis (neg. MSE):\", best_score)\n",
    "\n",
    "# Pipeline starten\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
