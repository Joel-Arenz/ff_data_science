{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "import nfl_data_py as nfl\n",
    "\n",
    "# data loading and plotting\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# models\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor, plot_importance\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# interpretation\n",
    "import shap\n",
    "from interpret import show\n",
    "\n",
    "# pipeline\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.feature_selection import RFECV, RFE\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder, OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, root_mean_squared_error, r2_score, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, nan_euclidean_distances\n",
    "\n",
    "pd.set_option('display.max_columns', None)  # None zeigt alle Spalten\n",
    "pd.set_option('display.max_rows', None)  # Alle Zeilen anzeigen, vorsichtig bei großen DataFrames\n",
    "pd.set_option('display.width', 1000)  # Breite anpassen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positionsspezifische Stats zusammengefasst (passing, rushing und receiving zu total_features zusammengefasst)\n",
    "# Alle Metriken (ewm, mean, median, min, max, std)\n",
    "# --> performed soweit am besten\n",
    "\n",
    "def load_and_merge_data():\n",
    "    df_ids = nfl.import_ids()\n",
    "    df_weekly = nfl.import_weekly_data(list(range(2018, 2025)))\n",
    "    df_seasonal = nfl.import_seasonal_data(list(range(2017,2024)))\n",
    "    df_schedule = nfl.import_schedules(list(range(2018, 2025)))\n",
    "    df_pass_pfr = nfl.import_weekly_pfr('pass', list(range(2018, 2025)))\n",
    "    df_rush_pfr = nfl.import_weekly_pfr('rush', list(range(2018, 2025)))\n",
    "    df_rec_pfr = nfl.import_weekly_pfr('rec', list(range(2018, 2025)))\n",
    "    df_pass_ngs = nfl.import_ngs_data('passing',list(range(2018, 2025)))\n",
    "    df_rush_ngs = nfl.import_ngs_data('rushing',list(range(2018, 2025)))\n",
    "    df_snap_counts = nfl.import_snap_counts(list(range(2018, 2025)))\n",
    "\n",
    "    df_weekly = df_weekly[(df_weekly['season_type'] == 'REG') & (df_weekly['position'].isin(['QB', 'WR', 'RB', 'TE']))].reset_index()\n",
    "\n",
    "    df_seasonal['season'] = df_seasonal['season'] + 1\n",
    "\n",
    "    df_schedule = df_schedule[['game_id', 'home_team', 'home_score', 'away_score']].drop_duplicates()\n",
    "    df_schedule['game_id'] = df_schedule['game_id'].str.replace('OAK', 'LV', regex=False) # Umzug der Oakland Raiders nach Las Vegas in der Saison 2020\n",
    "    df_schedule['home_team'] = df_schedule['home_team'].str.replace('OAK', 'LV', regex=False) # Umzug der Oakland Raiders nach Las Vegas in der Saison 2020\n",
    "\n",
    "    df_weekly['game_id_home_away'] = df_weekly['season'].astype(str) + '_' + df_weekly['week'].apply(lambda x: f\"{x:02d}\")+'_'+df_weekly['recent_team']+'_'+df_weekly['opponent_team']\n",
    "    df_weekly['game_id_away_home'] = df_weekly['season'].astype(str) + '_' + df_weekly['week'].apply(lambda x: f\"{x:02d}\")+'_'+df_weekly['opponent_team']+'_'+df_weekly['recent_team']\n",
    "\n",
    "    df_merged = pd.melt(\n",
    "        df_weekly,\n",
    "        id_vars=['player_id', 'position', 'season', 'week', 'recent_team', 'opponent_team', 'completions', 'attempts', 'passing_yards', 'passing_tds', 'passing_2pt_conversions', 'interceptions', 'sack_fumbles_lost', 'sacks', 'sack_yards', 'passing_air_yards', 'passing_epa', 'pacr', 'carries', 'rushing_yards', 'rushing_tds', 'rushing_2pt_conversions', 'rushing_fumbles_lost', 'rushing_epa', 'receptions', 'targets', 'receiving_yards', 'receiving_tds', 'receiving_2pt_conversions', 'receiving_fumbles_lost', 'racr', 'wopr', 'receiving_epa', 'fantasy_points'],\n",
    "        value_vars=['game_id_home_away', 'game_id_away_home'],\n",
    "        var_name='game_id_type',\n",
    "        value_name='game_id'\n",
    "    )\n",
    "\n",
    "    df_ids = df_ids.rename(columns={'gsis_id': 'player_id', 'pfr_id': 'pfr_player_id'})\n",
    "    df_pass_ngs = df_pass_ngs.rename(columns={'player_gsis_id': 'player_id'})\n",
    "    df_rush_ngs = df_rush_ngs.rename(columns={'player_gsis_id': 'player_id'})\n",
    "\n",
    "    df_merged = pd.merge(df_merged, df_schedule, on='game_id', how='inner') # Bei ein paar Spielen: recent_team = opponent_team\n",
    "    df_merged = pd.merge(df_merged, df_ids[['player_id', 'pfr_player_id', 'draft_pick', 'draft_year']], on = 'player_id', how = 'inner') # Ein paar Spieler ohne draft_year\n",
    "    df_merged = pd.merge(df_merged, df_seasonal[['player_id', 'season', 'dom']], on = ['player_id', 'season'], how = 'left')\n",
    "    df_merged = pd.merge(df_merged, df_pass_pfr[['pfr_player_id', 'season', 'week', 'passing_bad_throws', 'times_pressured']], on = ['pfr_player_id', 'season', 'week'], how = 'left')\n",
    "    df_merged = pd.merge(df_merged, df_rec_pfr[['pfr_player_id', 'season', 'week', 'receiving_rat']], on = ['pfr_player_id', 'season', 'week'], how = 'left')\n",
    "    df_merged = pd.merge(df_merged, df_rush_pfr[['pfr_player_id', 'season', 'week', 'rushing_broken_tackles']], on = ['pfr_player_id', 'season', 'week'], how = 'left')\n",
    "    df_merged = pd.merge(df_merged, df_pass_ngs[['player_id', 'season', 'week', 'passer_rating', 'aggressiveness']], on = ['player_id', 'season', 'week'], how = 'left')\n",
    "    df_merged = pd.merge(df_merged, df_rush_ngs[['player_id', 'season', 'week', 'efficiency']], on = ['player_id', 'season', 'week'], how = 'left')\n",
    "    df_merged = pd.merge(df_merged, df_snap_counts[['pfr_player_id', 'season', 'week', 'offense_snaps']], on = ['pfr_player_id', 'season', 'week'], how = 'left')\n",
    "\n",
    "    df_merged = df_merged.drop(columns=['game_id_type', 'pfr_player_id'])\n",
    "    return df_merged\n",
    "\n",
    "\n",
    "\n",
    "def edit_data(df_merged):\n",
    "    df_merged['draft_pick'] = df_merged['draft_pick'].fillna(260)\n",
    "    df_merged = df_merged.fillna(0)\n",
    "\n",
    "    df_merged['rookie_flag'] = (df_merged['season'] == df_merged['draft_year']).astype(int)\n",
    "    df_merged['last_season_data_flag'] = (df_merged['week'] < 6).astype(int)\n",
    "    df_merged['home'] = (df_merged['home_team'] == df_merged['recent_team']).astype(int)\n",
    "    df_merged['player_id'] = df_merged['player_id'].str.replace('00-00', '').astype(int)\n",
    "\n",
    "    # interceptions und fumbles als eigene features statt als turnover aggregiert\n",
    "    df_merged['turnover'] = (\n",
    "        df_merged['interceptions'] +\n",
    "        df_merged['sack_fumbles_lost'] +\n",
    "        df_merged['rushing_fumbles_lost'] +\n",
    "        df_merged['receiving_fumbles_lost']\n",
    "    )\n",
    "\n",
    "    # total epa aggregiert statt passing, rushing und receiving einzeln\n",
    "    df_merged['epa_total'] = (\n",
    "        df_merged['passing_epa'] + \n",
    "        df_merged['rushing_epa'] + \n",
    "        df_merged['receiving_epa']\n",
    "    )\n",
    "\n",
    "    # total points aggregiert statt passing, rushing und receiving tds und 2pt conversions einzeln\n",
    "    df_merged['points_total'] = (\n",
    "        (df_merged['rushing_tds'] * 6) + \n",
    "        (df_merged['rushing_2pt_conversions'] * 2) + \n",
    "        (df_merged['receiving_tds'] * 6) + \n",
    "        (df_merged['receiving_2pt_conversions'] * 2) + \n",
    "        (df_merged['passing_tds'] * 6) + \n",
    "        (df_merged['passing_2pt_conversions'] * 2)\n",
    "    )\n",
    "\n",
    "    # total yards aggregiert statt passing, rushing und receiving einzeln\n",
    "    df_merged['yards_total'] = (\n",
    "        df_merged['passing_yards'] +\n",
    "        df_merged['rushing_yards'] +\n",
    "        df_merged['receiving_yards']\n",
    "    )\n",
    "\n",
    "    # position target-encoded\n",
    "    position_means = df_merged.groupby(['position', 'season', 'week'])['fantasy_points'].mean().reset_index()\n",
    "    position_means.rename(columns={'fantasy_points': 'position_encoded'}, inplace=True)\n",
    "    df_merged = pd.merge(df_merged, position_means, on=['position', 'season', 'week'], how='left')\n",
    "\n",
    "    return df_merged\n",
    "\n",
    "\n",
    "\n",
    "def create_rolling_features(df_merged):  \n",
    "\n",
    "    # points_scored und points_allowed als Maßstab für Stärke eines Teams\n",
    "    df_merged['recent_team_points_scored'] = df_merged.apply(lambda row: row['home_score'] if row['home'] == 1 else row['away_score'], axis=1)\n",
    "    df_merged['opponent_team_points_allowed'] = df_merged['recent_team_points_scored']\n",
    "\n",
    "    df_unique_opponent_team_points_allowed = df_merged.drop_duplicates(subset=['game_id', 'opponent_team', 'opponent_team_points_allowed'])\n",
    "    df_unique_recent_team_points_scored = df_merged.drop_duplicates(subset=['game_id', 'recent_team', 'recent_team_points_scored'])\n",
    "\n",
    "    df_unique_opponent_team_points_allowed = df_unique_opponent_team_points_allowed.sort_values(by=['opponent_team', 'season', 'week']).reset_index(drop=True)\n",
    "    df_unique_recent_team_points_scored = df_unique_recent_team_points_scored.sort_values(by=['recent_team', 'season', 'week']).reset_index(drop=True)\n",
    "\n",
    "    df_unique_opponent_team_points_allowed['ewm_opponent_team_points_allowed_l5w'] = (\n",
    "        df_unique_opponent_team_points_allowed.groupby('opponent_team')['opponent_team_points_allowed']\n",
    "        .apply(lambda x: x.shift(1).ewm(span=5, min_periods=5).mean())\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n",
    "    for metric in ['mean', 'median', 'std']:\n",
    "            df_unique_opponent_team_points_allowed[f\"{metric}_opponent_team_points_allowed_l5w\"] = (\n",
    "                df_unique_opponent_team_points_allowed.groupby('opponent_team')['opponent_team_points_allowed']\n",
    "                .apply(lambda x: x.shift(1).rolling(window=5, min_periods=5).agg(metric))  # shift(1) schließt aktuelle Woche aus\n",
    "                .reset_index(level=0, drop=True)  # Index zurücksetzen\n",
    "        )\n",
    "\n",
    "    for metric in ['min', 'max']:\n",
    "            df_unique_opponent_team_points_allowed[f\"{metric}_opponent_team_points_allowed_l3w\"] = (\n",
    "                df_unique_opponent_team_points_allowed.groupby('opponent_team')['opponent_team_points_allowed']\n",
    "                .apply(lambda x: x.shift(1).rolling(window=3, min_periods=3).agg(metric))  # shift(1) schließt aktuelle Woche aus\n",
    "                .reset_index(level=0, drop=True)  # Index zurücksetzen\n",
    "        )\n",
    "\n",
    "    df_unique_opponent_team_points_allowed = df_unique_opponent_team_points_allowed.drop(columns=['player_id', 'draft_year', 'turnover', 'interceptions', 'sack_fumbles_lost', 'rushing_fumbles_lost', 'receiving_fumbles_lost', 'points_total', 'rushing_tds', 'rushing_2pt_conversions', 'receiving_tds', 'receiving_2pt_conversions', 'passing_tds', 'passing_2pt_conversions', 'epa_total', 'passing_epa', 'rushing_epa', 'receiving_epa', 'position', 'season', 'week', 'recent_team', 'home_team', 'completions', 'attempts', 'passing_yards', 'sacks', 'sack_yards', 'passing_air_yards', 'pacr', 'carries', 'rushing_yards', 'receptions', 'targets', 'yards_total', 'receiving_yards', 'racr', 'wopr', 'fantasy_points', 'home_score', 'away_score', 'draft_pick', 'dom', 'passing_bad_throws', 'times_pressured', 'receiving_rat', 'rushing_broken_tackles', 'passer_rating', 'aggressiveness', 'efficiency', 'offense_snaps', 'rookie_flag', 'last_season_data_flag', 'home', 'position_encoded', 'recent_team_points_scored', 'opponent_team_points_allowed'])\n",
    "    df_merged = pd.merge(df_merged, df_unique_opponent_team_points_allowed, on=['game_id','opponent_team'], how='inner')\n",
    "\n",
    "    df_unique_recent_team_points_scored['ewm_recent_team_points_scored_l5w'] = (\n",
    "        df_unique_recent_team_points_scored.groupby('recent_team')['recent_team_points_scored']\n",
    "        .apply(lambda x: x.shift(1).ewm(span=5, min_periods=5).mean())\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n",
    "    for metric in ['mean', 'median', 'std']:\n",
    "            df_unique_recent_team_points_scored[f\"{metric}_recent_team_points_scored_l5w\"] = (\n",
    "                df_unique_recent_team_points_scored.groupby('recent_team')['recent_team_points_scored']\n",
    "                .apply(lambda x: x.shift(1).rolling(window=5, min_periods=5).agg(metric))  # shift(1) schließt aktuelle Woche aus\n",
    "                .reset_index(level=0, drop=True)  # Index zurücksetzen\n",
    "        )\n",
    "\n",
    "    for metric in ['min', 'max']:\n",
    "            df_unique_recent_team_points_scored[f\"{metric}_recent_team_points_scored_l3w\"] = (\n",
    "                df_unique_recent_team_points_scored.groupby('recent_team')['recent_team_points_scored']\n",
    "                .apply(lambda x: x.shift(1).rolling(window=3, min_periods=3).agg(metric))  # shift(1) schließt aktuelle Woche aus\n",
    "                .reset_index(level=0, drop=True)  # Index zurücksetzen\n",
    "        )\n",
    "\n",
    "    df_unique_recent_team_points_scored = df_unique_recent_team_points_scored.drop(columns=['player_id', 'draft_year', 'turnover', 'interceptions', 'sack_fumbles_lost', 'rushing_fumbles_lost', 'receiving_fumbles_lost', 'points_total', 'rushing_tds', 'rushing_2pt_conversions', 'receiving_tds', 'receiving_2pt_conversions', 'passing_tds', 'passing_2pt_conversions', 'epa_total', 'passing_epa', 'rushing_epa', 'receiving_epa', 'position', 'season', 'week', 'opponent_team', 'home_team', 'completions', 'attempts', 'yards_total', 'passing_yards', 'sacks', 'sack_yards', 'passing_air_yards', 'pacr', 'carries', 'rushing_yards', 'receptions', 'targets', 'receiving_yards', 'racr', 'wopr', 'fantasy_points', 'home_score', 'away_score', 'draft_pick', 'dom', 'passing_bad_throws', 'times_pressured', 'receiving_rat', 'rushing_broken_tackles', 'passer_rating', 'aggressiveness', 'efficiency', 'offense_snaps', 'rookie_flag', 'last_season_data_flag', 'home', 'position_encoded', 'recent_team_points_scored', 'opponent_team_points_allowed'])\n",
    "    df_merged = pd.merge(df_merged, df_unique_recent_team_points_scored, on=['game_id','recent_team'], how='inner')\n",
    "\n",
    "    # Liste der Spalten mit Spielerspezifischen numerischen Daten, für die Rolling-Features erstellt werden sollen\n",
    "    columns_to_roll = ['completions', 'attempts', 'sacks', 'passer_rating', 'aggressiveness', 'efficiency', 'sack_yards', \n",
    "                    'passing_air_yards', 'pacr', 'carries', 'offense_snaps', 'yards_total', 'receptions', 'targets',\n",
    "                    'racr', 'wopr', 'fantasy_points', 'passing_bad_throws', 'times_pressured', 'position_encoded', 'receiving_rat', \n",
    "                    'rushing_broken_tackles', 'turnover', 'points_total', 'epa_total']\n",
    "\n",
    "\n",
    "    # Sortiere nach player_id, season und week\n",
    "    df_merged = df_merged.sort_values(by=['player_id', 'season', 'week']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    df_merged['cnt_games_over_20ffpts_l5w'] = (\n",
    "        df_merged.groupby('player_id')['fantasy_points']\n",
    "        .apply(lambda x: x.shift(1).rolling(window=5, min_periods=5).apply(lambda y: (y > 20).sum()))\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n",
    "    # Rolling-Features erstellen\n",
    "    for col in columns_to_roll:\n",
    "\n",
    "        feature_name_1 = f\"ewm_{col}_l5w\"\n",
    "        df_merged[feature_name_1] = (\n",
    "            df_merged.groupby('player_id')[col]\n",
    "            .apply(lambda x: x.shift(1).ewm(span=5, min_periods=5).mean())\n",
    "            .reset_index(level=0, drop=True)\n",
    "        )\n",
    "\n",
    "        for metric in ['mean', 'median', 'std']:\n",
    "            feature_name_2 = f\"{metric}_{col}_l5w\"\n",
    "            rolling_result_5w = (\n",
    "                df_merged.groupby('player_id')[col]\n",
    "                    .apply(lambda x: x.shift(1).rolling(window=5, min_periods=5).agg(metric))  # shift(1) schließt aktuelle Woche aus\n",
    "                    .reset_index(level=0, drop=True)  # Index zurücksetzen\n",
    "            )\n",
    "            # Einfügen der Rolling-Metrik\n",
    "            df_merged[feature_name_2] = rolling_result_5w\n",
    "\n",
    "        for metric in ['max', 'min']:\n",
    "            feature_name_3 = f\"{metric}_{col}_l3w\"\n",
    "            # Berechnung der Rolling-Metrik (ohne aktuelle Woche)\n",
    "            rolling_result_3w = (\n",
    "                df_merged.groupby('player_id')[col]\n",
    "                    .apply(lambda x: x.shift(1).rolling(window=3, min_periods=3).agg(metric))  # shift(1) schließt aktuelle Woche aus\n",
    "                    .reset_index(level=0, drop=True)  # Index zurücksetzen\n",
    "            )\n",
    "            # Einfügen der Rolling-Metrik\n",
    "            df_merged[feature_name_3] = rolling_result_3w\n",
    "\n",
    "    df_merged = df_merged.drop(columns=['completions', 'attempts', 'passing_yards', 'sacks', 'sack_yards', 'passing_air_yards', 'pacr', 'carries', \n",
    "                                        'rushing_yards', 'receptions', 'targets', 'receiving_yards', 'racr', 'wopr', 'passing_bad_throws', \n",
    "                                        'times_pressured', 'receiving_rat', 'rushing_broken_tackles', 'draft_year', 'home_team', \n",
    "                                        'passer_rating', 'aggressiveness', 'efficiency', 'offense_snaps', 'game_id',  'interceptions', 'sack_fumbles_lost', \n",
    "                                        'rushing_fumbles_lost', 'receiving_fumbles_lost', 'rushing_tds', 'rushing_2pt_conversions', 'receiving_tds', \n",
    "                                        'receiving_2pt_conversions', 'passing_tds', 'passing_2pt_conversions', 'passing_epa', 'rushing_epa', 'receiving_epa',\n",
    "                                        'position_encoded', 'recent_team', 'opponent_team', 'position', 'home_score', 'away_score',\n",
    "                                        'recent_team_points_scored', 'opponent_team_points_allowed', 'turnover', 'points_total', 'yards_total',\n",
    "                                        'epa_total'])\n",
    "\n",
    "    df_merged = df_merged.dropna().reset_index(level=0, drop=True)\n",
    "\n",
    "    return df_merged\n",
    "\n",
    "\n",
    "\n",
    "def prepare_features():  \n",
    "\n",
    "    df_merged = load_and_merge_data()\n",
    "    df_merged = edit_data(df_merged)\n",
    "    df_merged = create_rolling_features(df_merged)\n",
    "\n",
    "    return df_merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df_merged):\n",
    "\n",
    "    X_train = df_merged[df_merged['season'].isin(list(range(2018, 2024)))].drop(columns=['fantasy_points'])\n",
    "    y_train = df_merged[df_merged['season'].isin(list(range(2018, 2024)))]['fantasy_points']\n",
    "\n",
    "    X_val = df_merged[df_merged['season']==2023].drop(columns=['fantasy_points'])\n",
    "    y_val = df_merged[df_merged['season']==2023]['fantasy_points']\n",
    "\n",
    "    X_test = df_merged[df_merged['season']==2024].drop(columns=['fantasy_points'])\n",
    "    y_test = df_merged[df_merged['season']==2024]['fantasy_points']\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_preprocessor(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    numeric_features = X_train.select_dtypes(include=['int32', 'int64', 'float32', 'float64']).columns\n",
    "    categorical_features = X_train.select_dtypes(include=['object']).columns\n",
    "\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return preprocessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_pipeline(X_train, X_test, y_train, y_test, preprocessor):\n",
    "    # Modelle definieren\n",
    "    models = {\n",
    "        \"LinearRegression\": LinearRegression(),\n",
    "        \"XGBoost\": XGBRegressor()\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        # Pipeline erstellen\n",
    "        pipeline = Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('model', model)\n",
    "        ])\n",
    "        \n",
    "        # Training\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        # Vorhersagen\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "        \n",
    "        # Evaluation\n",
    "        results[model_name] = {\n",
    "            \"R2\": r2_score(y_test, y_pred),\n",
    "            \"MAE\": mean_absolute_error(y_test, y_pred),\n",
    "            \"RMSE\": np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_grid_search(X_train, y_train, preprocessor, model_name=\"XGBoost\"):\n",
    "    \n",
    "    # Hyperparameter für XGBoost\n",
    "    xgb_grid = {\n",
    "        \"model__max_depth\": [3, 6, 7, None],\n",
    "        \"model__learning_rate\": [0.05, 0.1, 0.3],\n",
    "        \"model__n_estimators\": [100, 500, 1000]\n",
    "    }\n",
    "    \n",
    "    # Pipeline erstellen\n",
    "    model = XGBRegressor()\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    # GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=Pipeline, param_grid=xgb_grid, verbose=2, cv=TimeSeriesSplit(n_splits=3))\n",
    "\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    return grid_search.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downcasting floats.\n"
     ]
    },
    {
     "ename": "RemoteDisconnected",
     "evalue": "Remote end closed connection without response",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteDisconnected\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Pipeline starten\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 20\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[91], line 4\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# Daten laden und aufteilen\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m split_data(df)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Preprocessor erstellen\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[86], line 229\u001b[0m, in \u001b[0;36mprepare_features\u001b[0;34m()\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_features\u001b[39m():  \n\u001b[0;32m--> 229\u001b[0m     df_merged \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_merge_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m     df_merged \u001b[38;5;241m=\u001b[39m edit_data(df_merged)\n\u001b[1;32m    231\u001b[0m     df_merged \u001b[38;5;241m=\u001b[39m create_rolling_features(df_merged)\n",
      "Cell \u001b[0;32mIn[86], line 8\u001b[0m, in \u001b[0;36mload_and_merge_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m df_ids \u001b[38;5;241m=\u001b[39m nfl\u001b[38;5;241m.\u001b[39mimport_ids()\n\u001b[1;32m      7\u001b[0m df_weekly \u001b[38;5;241m=\u001b[39m nfl\u001b[38;5;241m.\u001b[39mimport_weekly_data(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2018\u001b[39m, \u001b[38;5;241m2025\u001b[39m)))\n\u001b[0;32m----> 8\u001b[0m df_seasonal \u001b[38;5;241m=\u001b[39m \u001b[43mnfl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_seasonal_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2017\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2024\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m df_schedule \u001b[38;5;241m=\u001b[39m nfl\u001b[38;5;241m.\u001b[39mimport_schedules(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2018\u001b[39m, \u001b[38;5;241m2025\u001b[39m)))\n\u001b[1;32m     10\u001b[0m df_pass_pfr \u001b[38;5;241m=\u001b[39m nfl\u001b[38;5;241m.\u001b[39mimport_weekly_pfr(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpass\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2018\u001b[39m, \u001b[38;5;241m2025\u001b[39m)))\n",
      "File \u001b[0;32m~/Desktop/Uni/Master/WiSe_24-25/DSAI/Projekt/venv/lib/python3.12/site-packages/nfl_data_py/__init__.py:340\u001b[0m, in \u001b[0;36mimport_seasonal_data\u001b[0;34m(years, s_type)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;66;03m# import weekly data\u001b[39;00m\n\u001b[1;32m    339\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://github.com/nflverse/nflverse-data/releases/download/player_stats/player_stats_\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 340\u001b[0m data \u001b[38;5;241m=\u001b[39m pandas\u001b[38;5;241m.\u001b[39mconcat([\u001b[43mpandas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m years])\n\u001b[1;32m    342\u001b[0m \u001b[38;5;66;03m# filter to appropriate season_type\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m s_type \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mALL\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/Uni/Master/WiSe_24-25/DSAI/Projekt/venv/lib/python3.12/site-packages/pandas/io/parquet.py:667\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    664\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    665\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[0;32m--> 667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Uni/Master/WiSe_24-25/DSAI/Projekt/venv/lib/python3.12/site-packages/pandas/io/parquet.py:396\u001b[0m, in \u001b[0;36mFastParquetImpl.read\u001b[0;34m(self, path, columns, filters, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m     parquet_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m fsspec\u001b[38;5;241m.\u001b[39mopen(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(storage_options \u001b[38;5;129;01mor\u001b[39;00m {}))\u001b[38;5;241m.\u001b[39mfs\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(path):\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;66;03m# use get_handle only when we are very certain that it is not a directory\u001b[39;00m\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[0;32m--> 396\u001b[0m     handles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    399\u001b[0m     path \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/Uni/Master/WiSe_24-25/DSAI/Projekt/venv/lib/python3.12/site-packages/pandas/io/common.py:728\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    725\u001b[0m     codecs\u001b[38;5;241m.\u001b[39mlookup_error(errors)\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[0;32m--> 728\u001b[0m ioargs \u001b[38;5;241m=\u001b[39m \u001b[43m_get_filepath_or_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    736\u001b[0m handle \u001b[38;5;241m=\u001b[39m ioargs\u001b[38;5;241m.\u001b[39mfilepath_or_buffer\n\u001b[1;32m    737\u001b[0m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[0;32m~/Desktop/Uni/Master/WiSe_24-25/DSAI/Projekt/venv/lib/python3.12/site-packages/pandas/io/common.py:384\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# assuming storage_options is to be interpreted as headers\u001b[39;00m\n\u001b[1;32m    383\u001b[0m req_info \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(filepath_or_buffer, headers\u001b[38;5;241m=\u001b[39mstorage_options)\n\u001b[0;32m--> 384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq_info\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m req:\n\u001b[1;32m    385\u001b[0m     content_encoding \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m content_encoding \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    387\u001b[0m         \u001b[38;5;66;03m# Override compression based on Content-Encoding header\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Uni/Master/WiSe_24-25/DSAI/Projekt/venv/lib/python3.12/site-packages/pandas/io/common.py:289\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;124;03mLazy-import wrapper for stdlib urlopen, as that imports a big chunk of\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;124;03mthe stdlib.\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py:215\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 215\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py:515\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    512\u001b[0m     req \u001b[38;5;241m=\u001b[39m meth(req)\n\u001b[1;32m    514\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murllib.Request\u001b[39m\u001b[38;5;124m'\u001b[39m, req\u001b[38;5;241m.\u001b[39mfull_url, req\u001b[38;5;241m.\u001b[39mdata, req\u001b[38;5;241m.\u001b[39mheaders, req\u001b[38;5;241m.\u001b[39mget_method())\n\u001b[0;32m--> 515\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n\u001b[1;32m    518\u001b[0m meth_name \u001b[38;5;241m=\u001b[39m protocol\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_response\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py:532\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m    531\u001b[0m protocol \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mtype\n\u001b[0;32m--> 532\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_open\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m    533\u001b[0m \u001b[43m                          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_open\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[1;32m    535\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py:492\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    491\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 492\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    494\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py:1392\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1391\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttps_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[0;32m-> 1392\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHTTPSConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1393\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py:1348\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n\u001b[1;32m   1347\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n\u001b[0;32m-> 1348\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1349\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m   1350\u001b[0m     h\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:1419\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1417\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1418\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1419\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1420\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1421\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:300\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreply:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mrepr\u001b[39m(line))\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line:\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;66;03m# Presumably, the server closed the connection before\u001b[39;00m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;66;03m# sending a valid response.\u001b[39;00m\n\u001b[0;32m--> 300\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RemoteDisconnected(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRemote end closed connection without\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    301\u001b[0m                              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m response\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    303\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mRemoteDisconnected\u001b[0m: Remote end closed connection without response"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    # Daten laden und aufteilen\n",
    "    df = prepare_features()\n",
    "    X_train, X_test, y_train, y_test = split_data(df)\n",
    "    \n",
    "    # Preprocessor erstellen\n",
    "    preprocessor = create_preprocessor(X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    # Modelle trainieren und evaluieren\n",
    "    results = train_and_evaluate_pipeline(X_train, X_test, y_train, y_test, preprocessor)\n",
    "    print(\"Evaluationsergebnisse:\", results)\n",
    "    \n",
    "    # Hyperparameter-Tuning\n",
    "    best_score = perform_grid_search(X_train, y_train, preprocessor)\n",
    "    print(\"Bestes Ergebnis (neg. MSE):\", best_score)\n",
    "\n",
    "# Pipeline starten\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
