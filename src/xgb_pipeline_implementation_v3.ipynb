{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "import nfl_data_py as nfl\n",
    "\n",
    "# data loading and plotting\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# models\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor, plot_importance\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# interpretation\n",
    "import shap\n",
    "from interpret import show\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# pipeline\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.feature_selection import RFECV, RFE\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder, OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, root_mean_squared_error, r2_score, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, nan_euclidean_distances\n",
    "\n",
    "pd.set_option('display.max_columns', None)  # None zeigt alle Spalten\n",
    "pd.set_option('display.max_rows', None)  # Alle Zeilen anzeigen, vorsichtig bei großen DataFrames\n",
    "pd.set_option('display.width', 1000)  # Breite anpassen\n",
    "\n",
    "# Positionsspezifische Stats zusammengefasst (passing, rushing und receiving zu total_features zusammengefasst)\n",
    "# Alle Metriken (ewm, mean, median, min, max, std)\n",
    "# --> performed soweit am besten\n",
    "\n",
    "def load_and_merge_data():\n",
    "\n",
    "    df_ids = nfl.import_ids()\n",
    "    df_weekly = nfl.import_weekly_data(list(range(2018, 2025)))\n",
    "    df_seasonal = nfl.import_seasonal_data(list(range(2017,2024)))\n",
    "    df_schedule = nfl.import_schedules(list(range(2018, 2025)))\n",
    "    df_pass_pfr = nfl.import_weekly_pfr('pass', list(range(2018, 2025)))\n",
    "    df_rush_pfr = nfl.import_weekly_pfr('rush', list(range(2018, 2025)))\n",
    "    df_rec_pfr = nfl.import_weekly_pfr('rec', list(range(2018, 2025)))\n",
    "    df_pass_ngs = nfl.import_ngs_data('passing',list(range(2018, 2025)))\n",
    "    df_rush_ngs = nfl.import_ngs_data('rushing',list(range(2018, 2025)))\n",
    "    df_snap_counts = nfl.import_snap_counts(list(range(2018, 2025)))\n",
    "\n",
    "    df_weekly = df_weekly[(df_weekly['season_type'] == 'REG') & (df_weekly['position'].isin(['QB', 'WR', 'RB', 'TE']))].reset_index()\n",
    "\n",
    "    df_seasonal['season'] = df_seasonal['season'] + 1\n",
    "\n",
    "    df_schedule = df_schedule[['game_id', 'home_team', 'home_score', 'away_score']].drop_duplicates()\n",
    "    df_schedule['game_id'] = df_schedule['game_id'].str.replace('OAK', 'LV', regex=False) # Umzug der Oakland Raiders nach Las Vegas in der Saison 2020\n",
    "    df_schedule['home_team'] = df_schedule['home_team'].str.replace('OAK', 'LV', regex=False) # Umzug der Oakland Raiders nach Las Vegas in der Saison 2020\n",
    "\n",
    "    df_weekly['game_id_home_away'] = df_weekly['season'].astype(str) + '_' + df_weekly['week'].apply(lambda x: f\"{x:02d}\")+'_'+df_weekly['recent_team']+'_'+df_weekly['opponent_team']\n",
    "    df_weekly['game_id_away_home'] = df_weekly['season'].astype(str) + '_' + df_weekly['week'].apply(lambda x: f\"{x:02d}\")+'_'+df_weekly['opponent_team']+'_'+df_weekly['recent_team']\n",
    "\n",
    "    df_merged = pd.melt(\n",
    "        df_weekly,\n",
    "        id_vars=['player_id', 'position', 'season', 'week', 'recent_team', 'opponent_team', 'completions', 'attempts', 'passing_yards', 'passing_tds', 'passing_2pt_conversions', 'interceptions', 'sack_fumbles_lost', 'sacks', 'sack_yards', 'passing_air_yards', 'passing_epa', 'pacr', 'carries', 'rushing_yards', 'rushing_tds', 'rushing_2pt_conversions', 'rushing_fumbles_lost', 'rushing_epa', 'receptions', 'targets', 'receiving_yards', 'receiving_tds', 'receiving_2pt_conversions', 'receiving_fumbles_lost', 'racr', 'wopr', 'receiving_epa', 'fantasy_points'],\n",
    "        value_vars=['game_id_home_away', 'game_id_away_home'],\n",
    "        var_name='game_id_type',\n",
    "        value_name='game_id'\n",
    "    )\n",
    "\n",
    "    df_ids = df_ids.rename(columns={'gsis_id': 'player_id', 'pfr_id': 'pfr_player_id'})\n",
    "    df_pass_ngs = df_pass_ngs.rename(columns={'player_gsis_id': 'player_id'})\n",
    "    df_rush_ngs = df_rush_ngs.rename(columns={'player_gsis_id': 'player_id'})\n",
    "\n",
    "    df_merged = pd.merge(df_merged, df_schedule, on='game_id', how='inner') # Bei ein paar Spielen: recent_team = opponent_team\n",
    "    df_merged = pd.merge(df_merged, df_ids[['player_id', 'pfr_player_id', 'draft_pick', 'draft_year']], on = 'player_id', how = 'inner') # Ein paar Spieler ohne draft_year\n",
    "    df_merged = pd.merge(df_merged, df_seasonal[['player_id', 'season', 'dom']], on = ['player_id', 'season'], how = 'left')\n",
    "    df_merged = pd.merge(df_merged, df_pass_pfr[['pfr_player_id', 'season', 'week', 'passing_bad_throws', 'times_pressured']], on = ['pfr_player_id', 'season', 'week'], how = 'left')\n",
    "    df_merged = pd.merge(df_merged, df_rec_pfr[['pfr_player_id', 'season', 'week', 'receiving_rat']], on = ['pfr_player_id', 'season', 'week'], how = 'left')\n",
    "    df_merged = pd.merge(df_merged, df_rush_pfr[['pfr_player_id', 'season', 'week', 'rushing_broken_tackles']], on = ['pfr_player_id', 'season', 'week'], how = 'left')\n",
    "    df_merged = pd.merge(df_merged, df_pass_ngs[['player_id', 'season', 'week', 'passer_rating', 'aggressiveness']], on = ['player_id', 'season', 'week'], how = 'left')\n",
    "    df_merged = pd.merge(df_merged, df_rush_ngs[['player_id', 'season', 'week', 'efficiency']], on = ['player_id', 'season', 'week'], how = 'left')\n",
    "    df_merged = pd.merge(df_merged, df_snap_counts[['pfr_player_id', 'season', 'week', 'offense_snaps']], on = ['pfr_player_id', 'season', 'week'], how = 'left')\n",
    "\n",
    "    df_merged = df_merged.drop(columns=['game_id_type', 'pfr_player_id'])\n",
    "\n",
    "    return df_merged\n",
    "\n",
    "\n",
    "\n",
    "def edit_data(df_merged):\n",
    "    df_merged['draft_pick'] = df_merged['draft_pick'].fillna(260)\n",
    "    df_merged = df_merged.fillna(0)\n",
    "\n",
    "    df_merged['rookie_flag'] = (df_merged['season'] == df_merged['draft_year']).astype(int)\n",
    "    df_merged['last_season_data_flag'] = (df_merged['week'] < 6).astype(int)\n",
    "    df_merged['home'] = (df_merged['home_team'] == df_merged['recent_team']).astype(int)\n",
    "    df_merged['player_id'] = df_merged['player_id'].str.replace('00-00', '').astype(int)\n",
    "\n",
    "    # interceptions und fumbles als eigene features statt als turnover aggregiert\n",
    "    df_merged['turnover'] = (\n",
    "        df_merged['interceptions'] +\n",
    "        df_merged['sack_fumbles_lost'] +\n",
    "        df_merged['rushing_fumbles_lost'] +\n",
    "        df_merged['receiving_fumbles_lost']\n",
    "    )\n",
    "\n",
    "    # total epa aggregiert statt passing, rushing und receiving einzeln\n",
    "    df_merged['epa_total'] = (\n",
    "        df_merged['passing_epa'] + \n",
    "        df_merged['rushing_epa'] + \n",
    "        df_merged['receiving_epa']\n",
    "    )\n",
    "\n",
    "    # total points aggregiert statt passing, rushing und receiving tds und 2pt conversions einzeln\n",
    "    df_merged['points_total'] = (\n",
    "        (df_merged['rushing_tds'] * 6) + \n",
    "        (df_merged['rushing_2pt_conversions'] * 2) + \n",
    "        (df_merged['receiving_tds'] * 6) + \n",
    "        (df_merged['receiving_2pt_conversions'] * 2) + \n",
    "        (df_merged['passing_tds'] * 6) + \n",
    "        (df_merged['passing_2pt_conversions'] * 2)\n",
    "    )\n",
    "\n",
    "    # total yards aggregiert statt passing, rushing und receiving einzeln\n",
    "    df_merged['yards_total'] = (\n",
    "        df_merged['passing_yards'] +\n",
    "        df_merged['rushing_yards'] +\n",
    "        df_merged['receiving_yards']\n",
    "    )\n",
    "\n",
    "    # position target-encoded\n",
    "    position_means = df_merged.groupby(['position', 'season', 'week'])['fantasy_points'].mean().reset_index()\n",
    "    position_means.rename(columns={'fantasy_points': 'position_encoded'}, inplace=True)\n",
    "    df_merged = pd.merge(df_merged, position_means, on=['position', 'season', 'week'], how='left')\n",
    "\n",
    "    return df_merged\n",
    "\n",
    "\n",
    "\n",
    "def create_rolling_features(df_merged):  \n",
    "\n",
    "    # points_scored und points_allowed als Maßstab für Stärke eines Teams\n",
    "    df_merged['recent_team_points_scored'] = df_merged.apply(lambda row: row['home_score'] if row['home'] == 1 else row['away_score'], axis=1)\n",
    "    df_merged['opponent_team_points_allowed'] = df_merged['recent_team_points_scored']\n",
    "\n",
    "    df_unique_opponent_team_points_allowed = df_merged.drop_duplicates(subset=['game_id', 'opponent_team', 'opponent_team_points_allowed'])\n",
    "    df_unique_recent_team_points_scored = df_merged.drop_duplicates(subset=['game_id', 'recent_team', 'recent_team_points_scored'])\n",
    "\n",
    "    df_unique_opponent_team_points_allowed = df_unique_opponent_team_points_allowed.sort_values(by=['opponent_team', 'season', 'week']).reset_index(drop=True)\n",
    "    df_unique_recent_team_points_scored = df_unique_recent_team_points_scored.sort_values(by=['recent_team', 'season', 'week']).reset_index(drop=True)\n",
    "\n",
    "    df_unique_opponent_team_points_allowed['ewm_opponent_team_points_allowed_l5w'] = (\n",
    "        df_unique_opponent_team_points_allowed.groupby('opponent_team')['opponent_team_points_allowed']\n",
    "        .apply(lambda x: x.shift(1).ewm(span=5, min_periods=5).mean())\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n",
    "    for metric in ['mean', 'median', 'std']:\n",
    "            df_unique_opponent_team_points_allowed[f\"{metric}_opponent_team_points_allowed_l5w\"] = (\n",
    "                df_unique_opponent_team_points_allowed.groupby('opponent_team')['opponent_team_points_allowed']\n",
    "                .apply(lambda x: x.shift(1).rolling(window=5, min_periods=5).agg(metric))  # shift(1) schließt aktuelle Woche aus\n",
    "                .reset_index(level=0, drop=True)  # Index zurücksetzen\n",
    "        )\n",
    "\n",
    "    for metric in ['min', 'max']:\n",
    "            df_unique_opponent_team_points_allowed[f\"{metric}_opponent_team_points_allowed_l3w\"] = (\n",
    "                df_unique_opponent_team_points_allowed.groupby('opponent_team')['opponent_team_points_allowed']\n",
    "                .apply(lambda x: x.shift(1).rolling(window=3, min_periods=3).agg(metric))  # shift(1) schließt aktuelle Woche aus\n",
    "                .reset_index(level=0, drop=True)  # Index zurücksetzen\n",
    "        )\n",
    "\n",
    "    df_unique_opponent_team_points_allowed = df_unique_opponent_team_points_allowed.drop(columns=['player_id', 'draft_year', 'turnover', 'interceptions', 'sack_fumbles_lost', 'rushing_fumbles_lost', 'receiving_fumbles_lost', 'points_total', 'rushing_tds', 'rushing_2pt_conversions', 'receiving_tds', 'receiving_2pt_conversions', 'passing_tds', 'passing_2pt_conversions', 'epa_total', 'passing_epa', 'rushing_epa', 'receiving_epa', 'position', 'season', 'week', 'recent_team', 'home_team', 'completions', 'attempts', 'passing_yards', 'sacks', 'sack_yards', 'passing_air_yards', 'pacr', 'carries', 'rushing_yards', 'receptions', 'targets', 'yards_total', 'receiving_yards', 'racr', 'wopr', 'fantasy_points', 'home_score', 'away_score', 'draft_pick', 'dom', 'passing_bad_throws', 'times_pressured', 'receiving_rat', 'rushing_broken_tackles', 'passer_rating', 'aggressiveness', 'efficiency', 'offense_snaps', 'rookie_flag', 'last_season_data_flag', 'home', 'position_encoded', 'recent_team_points_scored', 'opponent_team_points_allowed'])\n",
    "    df_merged = pd.merge(df_merged, df_unique_opponent_team_points_allowed, on=['game_id','opponent_team'], how='inner')\n",
    "\n",
    "    df_unique_recent_team_points_scored['ewm_recent_team_points_scored_l5w'] = (\n",
    "        df_unique_recent_team_points_scored.groupby('recent_team')['recent_team_points_scored']\n",
    "        .apply(lambda x: x.shift(1).ewm(span=5, min_periods=5).mean())\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n",
    "    for metric in ['mean', 'median', 'std']:\n",
    "            df_unique_recent_team_points_scored[f\"{metric}_recent_team_points_scored_l5w\"] = (\n",
    "                df_unique_recent_team_points_scored.groupby('recent_team')['recent_team_points_scored']\n",
    "                .apply(lambda x: x.shift(1).rolling(window=5, min_periods=5).agg(metric))  # shift(1) schließt aktuelle Woche aus\n",
    "                .reset_index(level=0, drop=True)  # Index zurücksetzen\n",
    "        )\n",
    "\n",
    "    for metric in ['min', 'max']:\n",
    "            df_unique_recent_team_points_scored[f\"{metric}_recent_team_points_scored_l3w\"] = (\n",
    "                df_unique_recent_team_points_scored.groupby('recent_team')['recent_team_points_scored']\n",
    "                .apply(lambda x: x.shift(1).rolling(window=3, min_periods=3).agg(metric))  # shift(1) schließt aktuelle Woche aus\n",
    "                .reset_index(level=0, drop=True)  # Index zurücksetzen\n",
    "        )\n",
    "\n",
    "    df_unique_recent_team_points_scored = df_unique_recent_team_points_scored.drop(columns=['player_id', 'draft_year', 'turnover', 'interceptions', 'sack_fumbles_lost', 'rushing_fumbles_lost', 'receiving_fumbles_lost', 'points_total', 'rushing_tds', 'rushing_2pt_conversions', 'receiving_tds', 'receiving_2pt_conversions', 'passing_tds', 'passing_2pt_conversions', 'epa_total', 'passing_epa', 'rushing_epa', 'receiving_epa', 'position', 'season', 'week', 'opponent_team', 'home_team', 'completions', 'attempts', 'yards_total', 'passing_yards', 'sacks', 'sack_yards', 'passing_air_yards', 'pacr', 'carries', 'rushing_yards', 'receptions', 'targets', 'receiving_yards', 'racr', 'wopr', 'fantasy_points', 'home_score', 'away_score', 'draft_pick', 'dom', 'passing_bad_throws', 'times_pressured', 'receiving_rat', 'rushing_broken_tackles', 'passer_rating', 'aggressiveness', 'efficiency', 'offense_snaps', 'rookie_flag', 'last_season_data_flag', 'home', 'position_encoded', 'recent_team_points_scored', 'opponent_team_points_allowed'])\n",
    "    df_merged = pd.merge(df_merged, df_unique_recent_team_points_scored, on=['game_id','recent_team'], how='inner')\n",
    "\n",
    "    # Liste der Spalten mit Spielerspezifischen numerischen Daten, für die Rolling-Features erstellt werden sollen\n",
    "    columns_to_roll = ['completions', 'attempts', 'sacks', 'passer_rating', 'aggressiveness', 'efficiency', 'sack_yards', \n",
    "                    'passing_air_yards', 'pacr', 'carries', 'offense_snaps', 'yards_total', 'receptions', 'targets',\n",
    "                    'racr', 'wopr', 'fantasy_points', 'passing_bad_throws', 'times_pressured', 'position_encoded', 'receiving_rat', \n",
    "                    'rushing_broken_tackles', 'turnover', 'points_total', 'epa_total']\n",
    "\n",
    "\n",
    "    # Sortiere nach player_id, season und week\n",
    "    df_merged = df_merged.sort_values(by=['player_id', 'season', 'week']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    df_merged['cnt_games_over_20ffpts_l5w'] = (\n",
    "        df_merged.groupby('player_id')['fantasy_points']\n",
    "        .apply(lambda x: x.shift(1).rolling(window=5, min_periods=5).apply(lambda y: (y > 20).sum()))\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "\n",
    "    # Rolling-Features erstellen\n",
    "    for col in columns_to_roll:\n",
    "\n",
    "        feature_name_1 = f\"ewm_{col}_l5w\"\n",
    "        df_merged[feature_name_1] = (\n",
    "            df_merged.groupby('player_id')[col]\n",
    "            .apply(lambda x: x.shift(1).ewm(span=5, min_periods=5).mean())\n",
    "            .reset_index(level=0, drop=True)\n",
    "        )\n",
    "\n",
    "        for metric in ['mean', 'median', 'std']:\n",
    "            feature_name_2 = f\"{metric}_{col}_l5w\"\n",
    "            rolling_result_5w = (\n",
    "                df_merged.groupby('player_id')[col]\n",
    "                    .apply(lambda x: x.shift(1).rolling(window=5, min_periods=5).agg(metric))  # shift(1) schließt aktuelle Woche aus\n",
    "                    .reset_index(level=0, drop=True)  # Index zurücksetzen\n",
    "            )\n",
    "            # Einfügen der Rolling-Metrik\n",
    "            df_merged[feature_name_2] = rolling_result_5w\n",
    "\n",
    "        for metric in ['max', 'min']:\n",
    "            feature_name_3 = f\"{metric}_{col}_l3w\"\n",
    "            # Berechnung der Rolling-Metrik (ohne aktuelle Woche)\n",
    "            rolling_result_3w = (\n",
    "                df_merged.groupby('player_id')[col]\n",
    "                    .apply(lambda x: x.shift(1).rolling(window=3, min_periods=3).agg(metric))  # shift(1) schließt aktuelle Woche aus\n",
    "                    .reset_index(level=0, drop=True)  # Index zurücksetzen\n",
    "            )\n",
    "            # Einfügen der Rolling-Metrik\n",
    "            df_merged[feature_name_3] = rolling_result_3w\n",
    "\n",
    "    df_merged = df_merged.drop(columns=['completions', 'attempts', 'passing_yards', 'sacks', 'sack_yards', 'passing_air_yards', 'pacr', 'carries', \n",
    "                                        'rushing_yards', 'receptions', 'targets', 'receiving_yards', 'racr', 'wopr', 'passing_bad_throws', \n",
    "                                        'times_pressured', 'receiving_rat', 'rushing_broken_tackles', 'draft_year', 'home_team', \n",
    "                                        'passer_rating', 'aggressiveness', 'efficiency', 'offense_snaps', 'game_id',  'interceptions', 'sack_fumbles_lost', \n",
    "                                        'rushing_fumbles_lost', 'receiving_fumbles_lost', 'rushing_tds', 'rushing_2pt_conversions', 'receiving_tds', \n",
    "                                        'receiving_2pt_conversions', 'passing_tds', 'passing_2pt_conversions', 'passing_epa', 'rushing_epa', 'receiving_epa',\n",
    "                                        'position_encoded', 'recent_team', 'opponent_team', 'position', 'home_score', 'away_score',\n",
    "                                        'recent_team_points_scored', 'opponent_team_points_allowed', 'turnover', 'points_total', 'yards_total',\n",
    "                                        'epa_total'])\n",
    "\n",
    "    df_merged = df_merged.dropna().reset_index(level=0, drop=True)\n",
    "\n",
    "    return df_merged\n",
    "\n",
    "\n",
    "\n",
    "def prepare_features():  \n",
    "\n",
    "    df_merged = load_and_merge_data()\n",
    "    df_merged = edit_data(df_merged)\n",
    "    df_merged = create_rolling_features(df_merged)\n",
    "\n",
    "    return df_merged\n",
    "\n",
    "\n",
    "\n",
    "def split_data(df_merged):\n",
    "\n",
    "    X_train = df_merged[df_merged['season'].isin(list(range(2018, 2024)))].drop(columns=['fantasy_points'])\n",
    "    y_train = df_merged[df_merged['season'].isin(list(range(2018, 2024)))]['fantasy_points']\n",
    "\n",
    "    X_val = df_merged[df_merged['season']==2023].drop(columns=['fantasy_points'])\n",
    "    y_val = df_merged[df_merged['season']==2023]['fantasy_points']\n",
    "\n",
    "    X_test = df_merged[df_merged['season']==2024].drop(columns=['fantasy_points'])\n",
    "    y_test = df_merged[df_merged['season']==2024]['fantasy_points']\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "\n",
    "def create_preprocessor(df_merged):\n",
    "\n",
    "    X = df_merged.drop(columns=['fantasy_points'])\n",
    "\n",
    "    numeric_features = X.select_dtypes(include=['int32', 'int64', 'float32', 'float64']).columns\n",
    "    categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return preprocessor\n",
    "\n",
    "\n",
    "\n",
    "def create_pipeline(model, preprocessor):\n",
    "\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "\n",
    "def optimize_hyperparameters(model, param_grid, X_train, y_train, preprocessor):\n",
    "\n",
    "    pipeline = create_pipeline(model, preprocessor)\n",
    "    \n",
    "    grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=TimeSeriesSplit(n_splits=3), verbose=2)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "    print(f\"Best cross-validation score: {grid_search.best_score_}\")\n",
    "    \n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "\n",
    "def evaluate_models(model, X_test, y_test):\n",
    "    metrics = {\n",
    "        'mean_absolute_error': mean_absolute_error,\n",
    "        'mean_squared_error': mean_squared_error,\n",
    "        'root_mean_squared_error': lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        'r2_score': r2_score,\n",
    "        'spearman_rank_correlation': lambda y_true, y_pred: spearmanr(y_true, y_pred)[0]\n",
    "    }\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    model_name = model.named_steps['model'].__class__.__name__\n",
    "    print(f\"Evaluation results for model: {model_name}\")\n",
    "    for metric_name, metric_func in metrics.items():\n",
    "        score = metric_func(y_test, y_pred)\n",
    "        print(f\"{metric_name}: {score}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "import shap\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "def plot_feature_importances(model, X_train):\n",
    "    model_name = model.named_steps['model'].__class__.__name__\n",
    "    print(f\"Plotting feature importances for model: {model_name}\")\n",
    "\n",
    "    # Plot feature importances using SHAP\n",
    "    explainer = shap.Explainer(model.named_steps['model'], X_train)\n",
    "    shap_values = explainer(X_train)\n",
    "\n",
    "    if model_name == 'XGBRegressor':\n",
    "        # Hole die Feature-Wichtigkeiten\n",
    "        feature_importances = model.named_steps['model'].feature_importances_\n",
    "        # Erstelle ein DataFrame für Features und deren Wichtigkeiten\n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'Feature': X_train.columns,\n",
    "            'Importance': feature_importances\n",
    "        })\n",
    "        # Sortiere die Feature-Wichtigkeiten absteigend\n",
    "        feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "        # Wähle die Top 20 Features\n",
    "        top_features = feature_importance_df.head(20)\n",
    "\n",
    "        # Plotten der Feature-Wichtigkeiten\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.barh(top_features['Feature'], top_features['Importance'])\n",
    "        plt.gca().invert_yaxis()  # Um die höchste Wichtigkeit oben anzuzeigen\n",
    "        plt.xlabel(\"Feature Importance\")\n",
    "        plt.ylabel(\"Features\")\n",
    "        plt.title(f\"Top 20 Feature Importances in {model_name}\")\n",
    "        plt.show()\n",
    "\n",
    "    # Summary plot\n",
    "    shap.summary_plot(shap_values, X_train)\n",
    "\n",
    "    # Bar plot\n",
    "    shap.summary_plot(shap_values, X_train, plot_type=\"bar\")\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "import joblib\n",
    "\n",
    "def save_model(model, model_name):\n",
    "    # Speicherpfad definieren\n",
    "    file_path = f\"models/{model_name}_model.pkl\"\n",
    "    joblib.dump(model, file_path)\n",
    "    print(f\"Modell '{model_name}' wurde unter '{file_path}' gespeichert.\")\n",
    "\n",
    "\n",
    "\n",
    "def load_model(model_name):\n",
    "    # Speicherpfad definieren\n",
    "    file_path = f\"models/{model_name}_model.pkl\"\n",
    "    model = joblib.load(file_path)\n",
    "    print(f\"Modell '{model_name}' wurde aus '{file_path}' geladen.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def predict_and_merge(model_name, df_merged):\n",
    "    # Load the model\n",
    "    model = load_model(model_name)\n",
    "    \n",
    "    # Filter the data for the 2024 season\n",
    "    df_test = df_merged[df_merged['season'] == 2024]\n",
    "    \n",
    "    # Prepare the data for prediction\n",
    "    X_test = df_test.drop(columns=['fantasy_points'])\n",
    "    y_test = df_test['fantasy_points']\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Create the output dataframe\n",
    "    df_output = df_test[['season', 'week', 'player_id', 'position', 'recent_team', 'opponent_team']].copy()\n",
    "    df_output['predicted_fantasy_points'] = y_pred\n",
    "    df_output['actual_fantasy_points'] = y_test\n",
    "\n",
    "    return df_output\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Prepare the data\n",
    "    df_merged = prepare_features()\n",
    "    X_train, X_test, y_train, y_test = split_data(df_merged)\n",
    "    preprocessor = create_preprocessor(df_merged)\n",
    "\n",
    "    # Define the models and their hyperparameters\n",
    "    models = {\n",
    "        'XGBoost': (XGBRegressor(), {\n",
    "            'model__n_estimators': [100, 500, 1000],\n",
    "            'model__max_depth': [3, 6, 7, None],\n",
    "            'model__learning_rate': [0.05, 0.1, 0.3]\n",
    "        }),\n",
    "        'LinearRegression': (LinearRegression(), {\n",
    "            'model__fit_intercept': [True, False],\n",
    "            'model__n_jobs': [1, -1]\n",
    "        })\n",
    "    }\n",
    "\n",
    "    # Run the pipeline for each model\n",
    "    for model_name, (model, param_grid) in models.items():\n",
    "        print(f\"Performing Grid Search for {model_name}...\")\n",
    "        best_model = optimize_hyperparameters(model, param_grid, X_train, y_train, preprocessor)\n",
    "        print(f\"Evaluating {model_name}...\")\n",
    "        evaluate_models(best_model, X_test, y_test)\n",
    "        plot_feature_importances(best_model, X_train)\n",
    "        save_model(best_model, model_name)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
